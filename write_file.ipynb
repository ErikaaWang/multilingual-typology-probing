{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============imports===================\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Set, Optional, Any\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "\n",
    "import pycountry\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "from collections import defaultdict\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "lang_code_dict_2to3 = {'ar':'ara', 'eu':'eus', 'ca':'cat', 'zh':'zho', 'en':'eng', 'fr':'fra', 'hi':'hin', 'mr':'mar', 'pt':'por', 'es':'spa', 'ta':'tam', 'ur':'urd', 'vi':'vie'}\n",
    "lang_code_dict_3to2 = {'ara':'ar', 'eus':'eu', 'cat':'ca', 'zho':'zh', 'eng':'en', 'fra':'fr', 'hin':'hi', 'mar':'mr', 'por':'pt', 'spa':'es', 'tam':'ta', 'urd':'ur', 'vie':'vi'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================args=====================\n",
    "checkpoints = ['best']\n",
    "# checkpoints = ['best', '10000', '50000', '100000', '150000', '200000', '250000']\n",
    "# checkpoints = ['best', '1000', '10000', '100000', '200000', '300000', '400000', '500000', '600000']\n",
    "random_baseline = False\n",
    "embedding_size = 1024\n",
    "top_k = 50\n",
    "# attributes = ['Gender', 'Number', 'POS']\n",
    "shapes = {'Gender':'^', 'Number':'o', 'POS':'s'}\n",
    "# attributes = ['Aspect', 'Case', 'Definiteness', 'Finiteness', 'Gender', 'Mood', 'Number',\\\n",
    "#              'Person', 'POS', 'Tense', 'Voice']\n",
    "attributes = ['POS']\n",
    "language = None\n",
    "show_plot = False\n",
    "experiment_name = 'last-layer'\n",
    "# layers = [1, 5, 9, 13, 17, 21, 25]\n",
    "layers = None\n",
    "model = 'bloom-560m'\n",
    "\n",
    "\n",
    "# cross-lingual eval result args\n",
    "output_file_path = 'xtreme/outputs-temp/udpos/'\n",
    "output_file_suffix = '-LR2e-5-epoch10-MaxLen128'\n",
    "langs = ['ar', 'eu', 'ca', 'zh', 'en', 'fr', 'hi', 'mr', 'pt', 'es', 'ta', 'ur', 'vi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================functions====================\n",
    "def convert_language_code(treebank_name):\n",
    "    \"\"\" Converts treebank names to language codes. \"\"\"\n",
    "    lang_name = treebank_name[3:].split(\"-\")[0].replace(\"_\", \" \")\n",
    "    lang = pycountry.languages.get(name=lang_name)\n",
    "\n",
    "    if lang is not None:\n",
    "        return lang.alpha_3.lower()\n",
    "\n",
    "    return \"unk\"\n",
    "\n",
    "def compute_overlap(data_raw, top_k):\n",
    "    mark_count: Dict[str, int] = {}  # num of languages logging that attribute\n",
    "    results: Dict[str, Counter] = {}\n",
    "    for run_config, dims in data_raw:\n",
    "        if run_config[\"embedding\"] != embedding:\n",
    "            continue\n",
    "\n",
    "        # Increment mark counting\n",
    "        if run_config[\"attribute\"] not in mark_count:\n",
    "            mark_count[run_config[\"attribute\"]] = 0\n",
    "\n",
    "        mark_count[run_config[\"attribute\"]] += 1\n",
    "\n",
    "        # Increment actual counters\n",
    "        if run_config[\"attribute\"] not in results:\n",
    "            results[run_config[\"attribute\"]] = Counter()\n",
    "\n",
    "        results[run_config[\"attribute\"]].update(dims[:top_k])\n",
    "\n",
    "    return results, mark_count\n",
    "\n",
    "\n",
    "def compute_similarity_for_attribute(attribute, data_raw, top_k, language_order: Optional[List[str]] = None):\n",
    "    data_list: List[Tuple[str, Set[int]]] = []\n",
    "    for run_config, dims in data_raw:\n",
    "        if run_config[\"embedding\"] != embedding:\n",
    "            continue\n",
    "\n",
    "        if run_config[\"attribute\"] != attribute:\n",
    "            continue\n",
    "\n",
    "        data_list.append((run_config[\"language\"], set(dims[:top_k])))\n",
    "\n",
    "    if not language_order:\n",
    "        data_list = sorted(data_list, key=lambda x: x[0])\n",
    "        return [x[0] for x in data_list], compute_similarity(data_list)\n",
    "    else:\n",
    "        data_list_dict = {k: v for k, v in data_list}\n",
    "        data_list_sorted = []\n",
    "        for x in language_order:\n",
    "            if x in data_list_dict:\n",
    "                data_list_sorted.append((x, data_list_dict[x]))\n",
    "\n",
    "        data_list = data_list_sorted\n",
    "        return [x[0] for x in data_list], compute_similarity(data_list)\n",
    "\n",
    "\n",
    "def compute_similarity_for_language(language, data_raw, top_k):\n",
    "    data_list: List[Tuple[str, Set[int]]] = []\n",
    "    for run_config, dims in data_raw:\n",
    "        if run_config[\"embedding\"] != embedding:\n",
    "            continue\n",
    "\n",
    "        if run_config[\"language\"] != language:\n",
    "            continue\n",
    "\n",
    "        data_list.append((run_config[\"attribute\"], set(dims[:top_k])))\n",
    "\n",
    "    data_list = sorted(data_list, key=lambda x: x[0])\n",
    "\n",
    "    return [x[0] for x in data_list], compute_similarity(data_list)\n",
    "\n",
    "\n",
    "def compute_jaccard_index(set_a: Set[int], set_b: Set[int]) -> float:\n",
    "    return len(set_a & set_b) / len(set_a | set_b)\n",
    "\n",
    "\n",
    "def compute_overlap_coefficient(set_a: Set[int], set_b: Set[int]) -> float:\n",
    "    return len(set_a & set_b) / min(len(set_a), len(set_b))\n",
    "\n",
    "\n",
    "def compute_similarity(data_list: List[Tuple[str, Set[int]]]):\n",
    "    num_items = len(data_list)\n",
    "    similarity_array = np.zeros((num_items, num_items))\n",
    "    extra_data = {\n",
    "        \"overlap\": np.empty((num_items, num_items), dtype=list),\n",
    "        \"overlap_num\": np.zeros((num_items, num_items)),\n",
    "    }\n",
    "    for idx_a, (group_a, dim_set_a) in enumerate(data_list):\n",
    "        for idx_b, (group_b, dim_set_b) in enumerate(data_list):\n",
    "            similarity_array[idx_a, idx_b] = compute_overlap_coefficient(dim_set_a, dim_set_b)\n",
    "            extra_data[\"overlap\"][idx_a, idx_b] = sorted(list(set(dim_set_a & dim_set_b)))\n",
    "            extra_data[\"overlap_num\"][idx_a, idx_b] = len(dim_set_a & dim_set_b)\n",
    "\n",
    "    return similarity_array, extra_data\n",
    "\n",
    "\n",
    "def compute_pvalues(overlap_num_matrix: np.array, p_val_dict: Dict[int, float]) -> np.array:\n",
    "    return np.vectorize(lambda x: p_val_dict[int(x)])(overlap_num_matrix)\n",
    "\n",
    "\n",
    "def build_statistical_significance_matrix(p_values_matrix, alpha=0.05, method=\"bonferroni\", symmetry=False):\n",
    "    num_rows = p_values_matrix.shape[0]\n",
    "    num_hypotheses = int(num_rows * (num_rows + 1) / 2) - num_rows\n",
    "    num_hypotheses = num_hypotheses if num_hypotheses > 0 else 999\n",
    "    alpha_bonferroni = alpha / num_hypotheses\n",
    "\n",
    "    if method == \"bonferroni\":\n",
    "        mask = np.tril(np.ones_like(p_values_matrix, dtype=bool), k=-1)\n",
    "        significance_matrix = (p_values_matrix < alpha_bonferroni) * mask\n",
    "    elif method == \"holm-bonferroni\":\n",
    "        mask = np.triu(np.ones_like(p_values_matrix)) * 9999.0\n",
    "        p_values_matrix += mask\n",
    "        p_values_matrix_flat = p_values_matrix.reshape(-1)\n",
    "        sorting_indices = p_values_matrix_flat.argsort()\n",
    "        unsorting_indices = sorting_indices.argsort()\n",
    "\n",
    "        sorted_p_values = p_values_matrix_flat[sorting_indices][:num_hypotheses]\n",
    "        alpha_holm = np.arange(1.0, num_hypotheses + 1.0)[::-1] ** -1 * alpha\n",
    "\n",
    "        broke = False\n",
    "        for k, (pval, alph) in enumerate(zip(sorted_p_values.tolist(), alpha_holm.tolist())):\n",
    "            if pval > alph:\n",
    "                broke = True\n",
    "                break\n",
    "\n",
    "        if not broke:\n",
    "            # Needed in case we never accepted the null hypothesis\n",
    "            k += 1\n",
    "\n",
    "        # k will be equal to the first index where we do NOT reject the null hypothesis.\n",
    "        # So we can accept the alternative hypothesis on all indices less than k\n",
    "        # e.g., if k == 0, we always accept the null hypothesis. If k == num_hypothesis\n",
    "        # we always reject the null hypothesis.\n",
    "        rejected_null_sorted = [True if idx < k else False for idx in range(num_hypotheses)]\n",
    "\n",
    "        # Pad remaining list with rejections\n",
    "        rejected_null_sorted.extend([False] * (num_rows ** 2 - num_hypotheses))\n",
    "\n",
    "        # Reverse sort\n",
    "        significance_matrix = np.array(rejected_null_sorted)[unsorting_indices].reshape(num_rows, num_rows)\n",
    "\n",
    "    if symmetry:\n",
    "        # Mirror along diagonal\n",
    "        significance_matrix = significance_matrix | significance_matrix.T\n",
    "\n",
    "    return significance_matrix\n",
    "\n",
    "\n",
    "def build_annotations_list(annotation_matrix):\n",
    "    n = annotation_matrix.shape[0]\n",
    "    annotation_list = []\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            if not annotation_matrix[x][y]:\n",
    "                continue\n",
    "\n",
    "            if x == y:\n",
    "                continue\n",
    "\n",
    "            annotation_list.append(\n",
    "                dict(\n",
    "                    x=x / n, y=y / n,\n",
    "                    xref='paper',\n",
    "                    yref='paper',\n",
    "                    text=\"■\",\n",
    "                    showarrow=False,\n",
    "                    xanchor=\"left\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    font=dict(color=\"rgb(236,136,106)\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return annotation_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ======================read f1 scores==============================\n",
    "# # Dict{checkpoint: avg_f1_score}\n",
    "# avg_f1_dict = {}\n",
    "# # Dict{ckpt: Dict{lang: f1_score}}\n",
    "# ckpt_lan_f1_dict: Dict[str, Dict[str, float]] = defaultdict(dict)\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(8, 5) )\n",
    "\n",
    "# for ckpt in checkpoints:\n",
    "#     f1_dict = {}\n",
    "#     if ckpt == 'best':\n",
    "#         output_file_name = os.path.join(output_file_path, model + output_file_suffix, 'test_results.txt')\n",
    "#     else:\n",
    "#         ckpt_affix = '-intermediate-global_step' + ckpt\n",
    "#         output_file_name = os.path.join(output_file_path, model + ckpt_affix + output_file_suffix, 'test_results.txt')\n",
    "#     with open(output_file_name, 'r') as f:\n",
    "#         while True:\n",
    "#             line = f.readline()\n",
    "#             if not line:\n",
    "#                 break\n",
    "#             if 'language' in line:\n",
    "#                 lang = line.split('=')[1].split('\\n')[0]\n",
    "#                 f1_score = float(f.readline().split(' = ')[1])\n",
    "#                 if lang in langs:\n",
    "#                     f1_dict[lang] = f1_score \n",
    "#         print(f1_dict)\n",
    "#         ckpt_lan_f1_dict[ckpt] = f1_dict\n",
    "#         avg_f1_score = sum(f1_dict.values())/len(f1_dict.values()) * 100\n",
    "#         avg_f1_dict[ckpt] = avg_f1_score\n",
    "\n",
    "# print(avg_f1_dict)\n",
    "# print(ckpt_lan_f1_dict)\n",
    "# # plotting\n",
    "# # ckpts, f1_scores = zip(*avg_f1_dict.items())\n",
    "# # ax1.set_xlabel('global steps')\n",
    "# # ax1.set_ylabel('F1 scores', color='r')\n",
    "# # ax1.tick_params(axis='y', labelcolor='r', grid_alpha=0.5)\n",
    "# # ax1.plot(ckpts, f1_scores, 'r-')                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop over checkpoints\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.12 0.06 0.16 0.06 0.04 0.08 0.1  0.04 0.08 0.08 0.02 0.04]\n",
      " [0.12 1.   0.02 0.08 0.06 0.08 0.12 0.06 0.1  0.08 0.08 0.08 0.06]\n",
      " [0.06 0.02 1.   0.12 0.1  0.08 0.08 0.04 0.04 0.06 0.06 0.   0.  ]\n",
      " [0.16 0.08 0.12 1.   0.04 0.06 0.   0.06 0.04 0.02 0.08 0.1  0.06]\n",
      " [0.06 0.06 0.1  0.04 1.   0.08 0.08 0.12 0.1  0.08 0.1  0.   0.06]\n",
      " [0.04 0.08 0.08 0.06 0.08 1.   0.06 0.06 0.08 0.06 0.16 0.   0.06]\n",
      " [0.08 0.12 0.08 0.   0.08 0.06 1.   0.06 0.   0.06 0.04 0.06 0.1 ]\n",
      " [0.1  0.06 0.04 0.06 0.12 0.06 0.06 1.   0.16 0.04 0.04 0.1  0.1 ]\n",
      " [0.04 0.1  0.04 0.04 0.1  0.08 0.   0.16 1.   0.06 0.02 0.02 0.04]\n",
      " [0.08 0.08 0.06 0.02 0.08 0.06 0.06 0.04 0.06 1.   0.04 0.   0.  ]\n",
      " [0.08 0.08 0.06 0.08 0.1  0.16 0.04 0.04 0.02 0.04 1.   0.06 0.04]\n",
      " [0.02 0.08 0.   0.1  0.   0.   0.06 0.1  0.02 0.   0.06 1.   0.04]\n",
      " [0.04 0.06 0.   0.06 0.06 0.06 0.1  0.1  0.04 0.   0.04 0.04 1.  ]]\n",
      "[0.06, 0.02, 1.0, 0.12, 0.1, 0.08, 0.08, 0.04, 0.04, 0.06, 0.06, 0.0, 0.0]\n",
      "<class 'list'>\n",
      "defaultdict(<class 'dict'>, {'POS': {'best': 0.06307692307692307}})\n",
      "defaultdict(<class 'dict'>, {'best': {'ar': 0.06, 'ca': 0.02, 'en': 1.0, 'eu': 0.12, 'fr': 0.1, 'hi': 0.08, 'mr': 0.08, 'pt': 0.04, 'es': 0.04, 'ta': 0.06, 'ur': 0.06, 'vi': 0.0, 'zh': 0.0}})\n"
     ]
    }
   ],
   "source": [
    "# ===========================read overlap ratios============================\n",
    "# Dict{attr: Dict{checkpoint: avg_overlap_rate}}\n",
    "avg_overlap_rates: Dict[str, Dict[str, float]] = defaultdict(dict)\n",
    "\n",
    "# Dict{ckpt: Dict{lang: overlap_rate_with_eng}}, this is to record the overlap rate with english only.\n",
    "POS_ovlp_dict: Dict[str, Dict[str, float]] = defaultdict(dict)\n",
    "\n",
    "if layers is not None:\n",
    "    print(\"Loop over layers\")\n",
    "    for layer in layers:\n",
    "        embedding = model\n",
    "        experiment_name = f\"inter-layer-{layer}\"\n",
    "        if layer == 25:\n",
    "            experiment_name = \"last-layer\"\n",
    "        DEFAULT_RESULTS_FOLDER = f\"results/{embedding}/{experiment_name}\"\n",
    "\n",
    "        # print(listdir(DEFAULT_RESULTS_FOLDER))\n",
    "        RESULTS = []\n",
    "        rel_treebanks = []\n",
    "        for lang in listdir(DEFAULT_RESULTS_FOLDER):\n",
    "            if lang == 'UD_Chinese-CFL':\n",
    "                continue\n",
    "            rel_treebanks.append(lang)\n",
    "            for attr in listdir(join(DEFAULT_RESULTS_FOLDER, lang)):\n",
    "                RESULTS.append((lang, attr))\n",
    "                \n",
    "        # embedding, attribute, language\n",
    "\n",
    "        DEFAULT_FILE_FORMAT = join(DEFAULT_RESULTS_FOLDER, \"{lang}/{attribute}/loginfo.json\")\n",
    "\n",
    "        results_raw: List[Tuple[Dict[str, Any], List[int]]] = []\n",
    "\n",
    "        for l, a in RESULTS:  # noqa\n",
    "            if l in rel_treebanks:\n",
    "                with open(DEFAULT_FILE_FORMAT.format(lang=l, attribute=a), \"r\") as h:\n",
    "                    data = json.load(h)\n",
    "\n",
    "                results_raw.append(\n",
    "                    (\n",
    "                        { \"embedding\": embedding, \"attribute\": a,\n",
    "                        \"language\": convert_language_code(l) },\n",
    "                        [d[\"iteration_dimension\"] for d in data if \"iteration_dimension\" in d]\n",
    "                        if not random_baseline else random.sample(range(embedding_size), k=top_k)\n",
    "                    )\n",
    "            )\n",
    "        # Compute p values\n",
    "        num_permutations = 1000000\n",
    "        p_vals_cache_file = f\"{embedding}_{top_k}_{num_permutations}_pvals.pkl\"\n",
    "        if not os.path.exists(p_vals_cache_file):\n",
    "            # Compute p-values for different similarities\n",
    "            # if embedding in [\"bert-base-multilingual-cased\", \"xlm-roberta-base\"]:\n",
    "            #     dimensionality = 768\n",
    "            # elif embedding == \"xlm-roberta-large\":\n",
    "            #     dimensionality = 1024\n",
    "            # elif 'bloom' in embedding:\n",
    "            #     dimensionality = 1024\n",
    "            # else:\n",
    "            #     raise Exception(\"Embedding does not exist\")\n",
    "            #     dimensionality = 300\n",
    "            dimensionality = embedding_size\n",
    "\n",
    "            reference_order = random.sample(list(range(dimensionality)), dimensionality)\n",
    "            reference_top_k = reference_order[:top_k]\n",
    "            similarities = []\n",
    "\n",
    "            # for i in tqdm(range(num_permutations)):\n",
    "            #     permuted_top_k = random.sample(reference_order, top_k)\n",
    "            #     similarities.append(compute_overlap_coefficient(set(reference_top_k), set(permuted_top_k)))\n",
    "            #     pvals = {}\n",
    "\n",
    "            # for i in range(top_k + 1):\n",
    "            #     observed_hypothesis = i / top_k  # What is overlap score greater than or equal to?\n",
    "            #     permutations_match = [s for s in similarities if s >= observed_hypothesis]\n",
    "            #     pval = len(permutations_match) / num_permutations\n",
    "            #     print(f\"P-value when sim >= {observed_hypothesis} (overlap >= {i} dims): {pval:.5f}\")\n",
    "            #     pvals[i] = pval\n",
    "\n",
    "            with open(p_vals_cache_file, \"wb\") as h:\n",
    "                pickle.dump(pvals, h)\n",
    "        else:\n",
    "            with open(p_vals_cache_file, \"rb\") as h:\n",
    "                pvals = pickle.load(h)\n",
    "\n",
    "        for attr in attributes:\n",
    "            labels, (similarity_matrix, extra_data) = compute_similarity_for_attribute(\n",
    "                attr, results_raw, top_k)\n",
    "            \n",
    "            # get the average pairwise overlap:\n",
    "            print(len(labels))\n",
    "            print(similarity_matrix)\n",
    "            avg_overlap_rates[attr][layer] = np.average(similarity_matrix[np.triu_indices(len(labels), k = 1)])\n",
    "            \n",
    "\n",
    "            x_labels = labels\n",
    "            y_labels = labels\n",
    "\n",
    "            p_values_matrix = compute_pvalues(extra_data[\"overlap_num\"], pvals)\n",
    "            annotation_matrix = build_statistical_significance_matrix(\n",
    "                p_values_matrix, alpha=0.05, method=\"holm-bonferroni\", symmetry=True)\n",
    "    \n",
    "    for attr, overlap_rates in avg_overlap_rates.items():\n",
    "        overlap_rates = sorted(overlap_rates.items()) \n",
    "        print(overlap_rates)\n",
    "        x, y = zip(*overlap_rates)\n",
    "        \n",
    "        plt.plot(x, y, f'o-', label=attr)\n",
    "\n",
    "\n",
    "elif checkpoints is not None:\n",
    "    print(\"Loop over checkpoints\")\n",
    "    for checkpoint in checkpoints:\n",
    "        embedding = f\"{model}-intermediate-global_step{checkpoint}\"\n",
    "        if checkpoint == 'best':\n",
    "            embedding = model\n",
    "        DEFAULT_RESULTS_FOLDER = f\"results/{embedding}/{experiment_name}\"\n",
    "\n",
    "        # print(listdir(DEFAULT_RESULTS_FOLDER))\n",
    "        RESULTS = []\n",
    "        rel_treebanks = []\n",
    "        for lang in listdir(DEFAULT_RESULTS_FOLDER):\n",
    "            if lang == 'UD_Chinese-CFL':\n",
    "                continue\n",
    "            rel_treebanks.append(lang)\n",
    "            for attr in listdir(join(DEFAULT_RESULTS_FOLDER, lang)):\n",
    "                RESULTS.append((lang, attr))\n",
    "                \n",
    "        # embedding, attribute, language\n",
    "\n",
    "        DEFAULT_FILE_FORMAT = join(DEFAULT_RESULTS_FOLDER, \"{lang}/{attribute}/loginfo.json\")\n",
    "\n",
    "        results_raw: List[Tuple[Dict[str, Any], List[int]]] = []\n",
    "\n",
    "        for l, a in RESULTS:  # noqa\n",
    "            if l in rel_treebanks:\n",
    "                with open(DEFAULT_FILE_FORMAT.format(lang=l, attribute=a), \"r\") as h:\n",
    "                    data = json.load(h)\n",
    "\n",
    "                results_raw.append(\n",
    "                    (\n",
    "                        { \"embedding\": embedding, \"attribute\": a,\n",
    "                        \"language\": convert_language_code(l) },\n",
    "                        [d[\"iteration_dimension\"] for d in data if \"iteration_dimension\" in d]\n",
    "                        if not random_baseline else random.sample(range(embedding_size), k=top_k)\n",
    "                    )\n",
    "            )\n",
    "        # Compute p values\n",
    "        num_permutations = 1000000\n",
    "        p_vals_cache_file = f\"{model}_{top_k}_{num_permutations}_pvals.pkl\"\n",
    "        if not os.path.exists(p_vals_cache_file):\n",
    "            # Compute p-values for different similarities\n",
    "            # if embedding in [\"bert-base-multilingual-cased\", \"xlm-roberta-base\"]:\n",
    "            #     dimensionality = 768\n",
    "            # elif embedding == \"xlm-roberta-large\":\n",
    "            #     dimensionality = 1024\n",
    "            # elif 'bloom' in embedding:\n",
    "            #     dimensionality = 1024\n",
    "            # else:\n",
    "            #     raise Exception(\"Embedding does not exist\")\n",
    "            #     dimensionality = 300\n",
    "            dimensionality = embedding_size\n",
    "\n",
    "            reference_order = random.sample(list(range(dimensionality)), dimensionality)\n",
    "            reference_top_k = reference_order[:top_k]\n",
    "            similarities = []\n",
    "\n",
    "            for i in tqdm(range(num_permutations)):\n",
    "                permuted_top_k = random.sample(reference_order, top_k)\n",
    "                similarities.append(compute_overlap_coefficient(set(reference_top_k), set(permuted_top_k)))\n",
    "                pvals = {}\n",
    "\n",
    "            for i in range(top_k + 1):\n",
    "                observed_hypothesis = i / top_k  # What is overlap score greater than or equal to?\n",
    "                permutations_match = [s for s in similarities if s >= observed_hypothesis]\n",
    "                pval = len(permutations_match) / num_permutations\n",
    "                print(f\"P-value when sim >= {observed_hypothesis} (overlap >= {i} dims): {pval:.5f}\")\n",
    "                pvals[i] = pval\n",
    "\n",
    "            with open(p_vals_cache_file, \"wb\") as h:\n",
    "                pickle.dump(pvals, h)\n",
    "        else:\n",
    "            with open(p_vals_cache_file, \"rb\") as h:\n",
    "                pvals = pickle.load(h)\n",
    "\n",
    "        for attr in attributes:\n",
    "            labels, (similarity_matrix, extra_data) = compute_similarity_for_attribute(\n",
    "                attr, results_raw, top_k)\n",
    "            \n",
    "            # get the average pairwise overlap:\n",
    "            print(labels)\n",
    "            print(similarity_matrix)\n",
    "            if attr == 'POS':\n",
    "                ovlp_rate_with_eng = similarity_matrix[labels.index('eng')].tolist()\n",
    "                print(ovlp_rate_with_eng)\n",
    "                print(type(ovlp_rate_with_eng))\n",
    "                for lang, ovlp_rate in zip(labels, ovlp_rate_with_eng):\n",
    "                    langto2 = lang_code_dict_3to2[lang]\n",
    "                    POS_ovlp_dict[checkpoint][langto2] = ovlp_rate\n",
    "\n",
    "            if checkpoint == 'best':\n",
    "                avg_overlap_rates[attr][checkpoint] = np.average(similarity_matrix[np.triu_indices(len(labels), k = 1)])\n",
    "            else:\n",
    "                avg_overlap_rates[attr][checkpoint] = np.average(similarity_matrix[np.triu_indices(len(labels), k = 1)])\n",
    "\n",
    "            x_labels = labels\n",
    "            y_labels = labels\n",
    "\n",
    "            p_values_matrix = compute_pvalues(extra_data[\"overlap_num\"], pvals)\n",
    "            annotation_matrix = build_statistical_significance_matrix(\n",
    "                p_values_matrix, alpha=0.05, method=\"holm-bonferroni\", symmetry=True)\n",
    "    \n",
    "    for attr, overlap_rates in avg_overlap_rates.items():\n",
    "        \n",
    "        if 'best' in overlap_rates.keys():\n",
    "            rate_for_best_checkpoint = [overlap_rates['best']]*8\n",
    "            # del overlap_rates['best']\n",
    "            # plt.plot(x, rate_for_best_checkpoint, f'-{color}', label=f'{model}-{attr}')\n",
    "            \n",
    "        # overlap_rates = sorted(overlap_rates.items()) \n",
    "        # x, y = zip(*overlap_rates)\n",
    "        \n",
    "        shape = shapes[attr]\n",
    "        \n",
    "        # ax2.plot(x, y, f'{shape}--', label=attr)      \n",
    "    \n",
    "else: \n",
    "    print(\"no x axises. please check.\")\n",
    "\n",
    "print(avg_overlap_rates)\n",
    "print(POS_ovlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(POS_ovlp_dict)\n",
    "df.to_csv(f'csv_files/{model}_{experiment_name}_ovlp-rate_POS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpts_avg_overlap_rates = {}\n",
    "\n",
    "# for ckpt in checkpoints:\n",
    "#     ckpt_sum_rates = 0\n",
    "#     for attr in attributes:\n",
    "#         ckpt_sum_rates += avg_overlap_rates[attr][ckpt]\n",
    "#     ckpts_avg_overlap_rates[ckpt] = ckpt_sum_rates/len(attributes)\n",
    "\n",
    "# print(ckpts_avg_overlap_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_category = pd.DataFrame.from_dict(avg_overlap_rates)\n",
    "# df_category.to_csv(f'csv_files/{model}_{experiment_name}_avg-ovlp-rate-by-ctgr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'csv_files/{model}_{experiment_name}_avg-ovlp-rate-by-ckpt.txt', 'w') as f:\n",
    "#     for ckpt, avg_rate in ckpts_avg_overlap_rates.items():\n",
    "#         f.write(ckpt + ',' + str(avg_rate) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"f1_rate_pairwise.txt\", 'a+') as f: \n",
    "#     for ckpt in checkpoints:\n",
    "#         for lang, f1_score in ckpt_lan_f1_dict[ckpt].items():\n",
    "#             f.write('%s %s %s\\n' % (f1_score, pos_ovlp_dict[ckpt][lang_code_dict[lang]], ckpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"f1_rate_ckpts.txt\", 'a+') as f: \n",
    "#     for ckpt in checkpoints:\n",
    "#         f.write('%s %s %s\\n' % (avg_f1_dict[ckpt], ckpts_avg_overlap_rates[ckpt], ckpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.legend(loc='center left', bbox_to_anchor=(1.2, 0.5))\n",
    "# ax2.set_ylabel('overlap rates')\n",
    "# fig.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "# # fig.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "# if layers is not None:\n",
    "#     fig.xlabel(\"layer\")\n",
    "    \n",
    "# plt.style.use('seaborn-darkgrid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.tight_layout()\n",
    "# sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if layers is not None:\n",
    "#     plt.savefig(f'experiments/scatterplots/{model}/layers.pdf')\n",
    "# elif checkpoints is not None:\n",
    "#     plt.savefig(f'experiments/scatterplots/{model}/checkpoints_{experiment_name}.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-typology-probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed44db9dbf3c575ec3ddf9da9744b6e2502eab3ed89ccbe6b1ade08a3a1a68bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
