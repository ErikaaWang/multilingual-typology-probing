{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============imports===================\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Set, Optional, Any\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "\n",
    "import pycountry\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "from collections import defaultdict\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "lang_code_dict = {'ar':'ara', 'eu':'eus', 'ca':'cat', 'zh':'zho', 'en':'eng', 'fr':'fra', 'hi':'hin', 'mr':'mar', 'pt':'por', 'es':'spa', 'ta':'tam', 'ur':'urd', 'vi':'vie'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================args=====================\n",
    "# checkpoints = 'best'\n",
    "checkpoints = ['best', '1000', '10000', '50000', '100000', '150000', '200000', '250000', '300000']\n",
    "# checkpoints = ['best', '1000', '10000', '100000', '200000', '300000', '400000', '500000', '600000']\n",
    "random_baseline = False\n",
    "embedding_size = 1536\n",
    "top_k = 50\n",
    "attributes = ['Gender', 'Number', 'POS']\n",
    "shapes = {'Gender':'^', 'Number':'o', 'POS':'s'}\n",
    "# attributes = ['Aspect', 'Case', 'Definiteness', 'Finiteness', 'Gender', 'Mood', 'Number',\\\n",
    "#              'Person', 'POS', 'Tense', 'Voice']\n",
    "language = None\n",
    "show_plot = False\n",
    "experiment_name = 'inter-layer-17'\n",
    "# layers = [1, 5, 9, 13, 17, 21, 25]\n",
    "layers = None\n",
    "model = 'bloom-1b7'\n",
    "\n",
    "\n",
    "# cross-lingual eval result args\n",
    "output_file_path = 'xtreme/outputs-temp/udpos/'\n",
    "output_file_suffix = '-LR2e-5-epoch10-MaxLen128'\n",
    "langs = ['ar', 'eu', 'ca', 'zh', 'en', 'fr', 'hi', 'mr', 'pt', 'es', 'ta', 'ur', 'vi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================functions====================\n",
    "def convert_language_code(treebank_name):\n",
    "    \"\"\" Converts treebank names to language codes. \"\"\"\n",
    "    lang_name = treebank_name[3:].split(\"-\")[0].replace(\"_\", \" \")\n",
    "    lang = pycountry.languages.get(name=lang_name)\n",
    "\n",
    "    if lang is not None:\n",
    "        return lang.alpha_3.lower()\n",
    "\n",
    "    return \"unk\"\n",
    "\n",
    "def compute_overlap(data_raw, top_k):\n",
    "    mark_count: Dict[str, int] = {}  # num of languages logging that attribute\n",
    "    results: Dict[str, Counter] = {}\n",
    "    for run_config, dims in data_raw:\n",
    "        if run_config[\"embedding\"] != embedding:\n",
    "            continue\n",
    "\n",
    "        # Increment mark counting\n",
    "        if run_config[\"attribute\"] not in mark_count:\n",
    "            mark_count[run_config[\"attribute\"]] = 0\n",
    "\n",
    "        mark_count[run_config[\"attribute\"]] += 1\n",
    "\n",
    "        # Increment actual counters\n",
    "        if run_config[\"attribute\"] not in results:\n",
    "            results[run_config[\"attribute\"]] = Counter()\n",
    "\n",
    "        results[run_config[\"attribute\"]].update(dims[:top_k])\n",
    "\n",
    "    return results, mark_count\n",
    "\n",
    "\n",
    "def compute_similarity_for_attribute(attribute, data_raw, top_k, language_order: Optional[List[str]] = None):\n",
    "    data_list: List[Tuple[str, Set[int]]] = []\n",
    "    for run_config, dims in data_raw:\n",
    "        if run_config[\"embedding\"] != embedding:\n",
    "            continue\n",
    "\n",
    "        if run_config[\"attribute\"] != attribute:\n",
    "            continue\n",
    "\n",
    "        data_list.append((run_config[\"language\"], set(dims[:top_k])))\n",
    "\n",
    "    if not language_order:\n",
    "        data_list = sorted(data_list, key=lambda x: x[0])\n",
    "        return [x[0] for x in data_list], compute_similarity(data_list)\n",
    "    else:\n",
    "        data_list_dict = {k: v for k, v in data_list}\n",
    "        data_list_sorted = []\n",
    "        for x in language_order:\n",
    "            if x in data_list_dict:\n",
    "                data_list_sorted.append((x, data_list_dict[x]))\n",
    "\n",
    "        data_list = data_list_sorted\n",
    "        return [x[0] for x in data_list], compute_similarity(data_list)\n",
    "\n",
    "\n",
    "def compute_similarity_for_language(language, data_raw, top_k):\n",
    "    data_list: List[Tuple[str, Set[int]]] = []\n",
    "    for run_config, dims in data_raw:\n",
    "        if run_config[\"embedding\"] != embedding:\n",
    "            continue\n",
    "\n",
    "        if run_config[\"language\"] != language:\n",
    "            continue\n",
    "\n",
    "        data_list.append((run_config[\"attribute\"], set(dims[:top_k])))\n",
    "\n",
    "    data_list = sorted(data_list, key=lambda x: x[0])\n",
    "\n",
    "    return [x[0] for x in data_list], compute_similarity(data_list)\n",
    "\n",
    "\n",
    "def compute_jaccard_index(set_a: Set[int], set_b: Set[int]) -> float:\n",
    "    return len(set_a & set_b) / len(set_a | set_b)\n",
    "\n",
    "\n",
    "def compute_overlap_coefficient(set_a: Set[int], set_b: Set[int]) -> float:\n",
    "    return len(set_a & set_b) / min(len(set_a), len(set_b))\n",
    "\n",
    "\n",
    "def compute_similarity(data_list: List[Tuple[str, Set[int]]]):\n",
    "    num_items = len(data_list)\n",
    "    similarity_array = np.zeros((num_items, num_items))\n",
    "    extra_data = {\n",
    "        \"overlap\": np.empty((num_items, num_items), dtype=list),\n",
    "        \"overlap_num\": np.zeros((num_items, num_items)),\n",
    "    }\n",
    "    for idx_a, (group_a, dim_set_a) in enumerate(data_list):\n",
    "        for idx_b, (group_b, dim_set_b) in enumerate(data_list):\n",
    "            similarity_array[idx_a, idx_b] = compute_overlap_coefficient(dim_set_a, dim_set_b)\n",
    "            extra_data[\"overlap\"][idx_a, idx_b] = sorted(list(set(dim_set_a & dim_set_b)))\n",
    "            extra_data[\"overlap_num\"][idx_a, idx_b] = len(dim_set_a & dim_set_b)\n",
    "\n",
    "    return similarity_array, extra_data\n",
    "\n",
    "\n",
    "def compute_pvalues(overlap_num_matrix: np.array, p_val_dict: Dict[int, float]) -> np.array:\n",
    "    return np.vectorize(lambda x: p_val_dict[int(x)])(overlap_num_matrix)\n",
    "\n",
    "\n",
    "def build_statistical_significance_matrix(p_values_matrix, alpha=0.05, method=\"bonferroni\", symmetry=False):\n",
    "    num_rows = p_values_matrix.shape[0]\n",
    "    num_hypotheses = int(num_rows * (num_rows + 1) / 2) - num_rows\n",
    "    num_hypotheses = num_hypotheses if num_hypotheses > 0 else 999\n",
    "    alpha_bonferroni = alpha / num_hypotheses\n",
    "\n",
    "    if method == \"bonferroni\":\n",
    "        mask = np.tril(np.ones_like(p_values_matrix, dtype=bool), k=-1)\n",
    "        significance_matrix = (p_values_matrix < alpha_bonferroni) * mask\n",
    "    elif method == \"holm-bonferroni\":\n",
    "        mask = np.triu(np.ones_like(p_values_matrix)) * 9999.0\n",
    "        p_values_matrix += mask\n",
    "        p_values_matrix_flat = p_values_matrix.reshape(-1)\n",
    "        sorting_indices = p_values_matrix_flat.argsort()\n",
    "        unsorting_indices = sorting_indices.argsort()\n",
    "\n",
    "        sorted_p_values = p_values_matrix_flat[sorting_indices][:num_hypotheses]\n",
    "        alpha_holm = np.arange(1.0, num_hypotheses + 1.0)[::-1] ** -1 * alpha\n",
    "\n",
    "        broke = False\n",
    "        for k, (pval, alph) in enumerate(zip(sorted_p_values.tolist(), alpha_holm.tolist())):\n",
    "            if pval > alph:\n",
    "                broke = True\n",
    "                break\n",
    "\n",
    "        if not broke:\n",
    "            # Needed in case we never accepted the null hypothesis\n",
    "            k += 1\n",
    "\n",
    "        # k will be equal to the first index where we do NOT reject the null hypothesis.\n",
    "        # So we can accept the alternative hypothesis on all indices less than k\n",
    "        # e.g., if k == 0, we always accept the null hypothesis. If k == num_hypothesis\n",
    "        # we always reject the null hypothesis.\n",
    "        rejected_null_sorted = [True if idx < k else False for idx in range(num_hypotheses)]\n",
    "\n",
    "        # Pad remaining list with rejections\n",
    "        rejected_null_sorted.extend([False] * (num_rows ** 2 - num_hypotheses))\n",
    "\n",
    "        # Reverse sort\n",
    "        significance_matrix = np.array(rejected_null_sorted)[unsorting_indices].reshape(num_rows, num_rows)\n",
    "\n",
    "    if symmetry:\n",
    "        # Mirror along diagonal\n",
    "        significance_matrix = significance_matrix | significance_matrix.T\n",
    "\n",
    "    return significance_matrix\n",
    "\n",
    "\n",
    "def build_annotations_list(annotation_matrix):\n",
    "    n = annotation_matrix.shape[0]\n",
    "    annotation_list = []\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            if not annotation_matrix[x][y]:\n",
    "                continue\n",
    "\n",
    "            if x == y:\n",
    "                continue\n",
    "\n",
    "            annotation_list.append(\n",
    "                dict(\n",
    "                    x=x / n, y=y / n,\n",
    "                    xref='paper',\n",
    "                    yref='paper',\n",
    "                    text=\"â– \",\n",
    "                    showarrow=False,\n",
    "                    xanchor=\"left\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    font=dict(color=\"rgb(236,136,106)\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return annotation_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ar': 0.4457881244011705, 'en': 0.8548907473714842, 'es': 0.7098428545706285, 'eu': 0.36745447905253825, 'fr': 0.6881672447482472, 'hi': 0.3109705343765607, 'mr': 0.35336976320582875, 'pt': 0.6822043321620779, 'ta': 0.3291139240506329, 'ur': 0.26611547721800755, 'vi': 0.4597988692650223, 'zh': 0.36931900752782976}\n",
      "{'ar': 0.20221559916774448, 'en': 0.8185548413323509, 'es': 0.30240275347270124, 'eu': 0.31663388883138394, 'fr': 0.29023683301545344, 'hi': 0.16683689847375024, 'mr': 0.3375, 'pt': 0.3567648256208441, 'ta': 0.21630615640599002, 'ur': 0.10973881352500506, 'vi': 0.30276564774381365, 'zh': 0.2941575771101254}\n",
      "{'ar': 0.20221559916774448, 'en': 0.8185548413323509, 'es': 0.30240275347270124, 'eu': 0.31663388883138394, 'fr': 0.29023683301545344, 'hi': 0.16683689847375024, 'mr': 0.3375, 'pt': 0.3567648256208441, 'ta': 0.21630615640599002, 'ur': 0.10973881352500506, 'vi': 0.30276564774381365, 'zh': 0.2941575771101254}\n",
      "{'ar': 0.27997624574852886, 'en': 0.8615588848852644, 'es': 0.7141992763913414, 'eu': 0.3600314938086035, 'fr': 0.6993381043577596, 'hi': 0.22057793802581388, 'mr': 0.21875, 'pt': 0.6776955910814179, 'ta': 0.12916666666666665, 'ur': 0.1452228069328441, 'vi': 0.4155240079461214, 'zh': 0.20177934431798877}\n",
      "{'ar': 0.22555688942243565, 'en': 0.8633667623588329, 'es': 0.711191605362411, 'eu': 0.3611718408395278, 'fr': 0.7146929116919436, 'hi': 0.2115011604309852, 'mr': 0.2495726495726496, 'pt': 0.6880511091140434, 'ta': 0.17717206132879046, 'ur': 0.13384935992855015, 'vi': 0.4361675817381412, 'zh': 0.09864774692576905}\n",
      "{'ar': 0.27279435016338144, 'en': 0.8600099907048506, 'es': 0.7205908935082744, 'eu': 0.3737298517860146, 'fr': 0.7254749069007704, 'hi': 0.2678132429561435, 'mr': 0.323679727427598, 'pt': 0.6880255455029272, 'ta': 0.2106490478041197, 'ur': 0.1443433029908973, 'vi': 0.4570058239122987, 'zh': 0.23637892774270688}\n",
      "{'ar': 0.18504734883378163, 'en': 0.8595506684297437, 'es': 0.7245445995445997, 'eu': 0.3806770268642332, 'fr': 0.7092537715517241, 'hi': 0.20405691795337574, 'mr': 0.22137404580152673, 'pt': 0.6845653040435802, 'ta': 0.22240000000000001, 'ur': 0.11960504885993486, 'vi': 0.4610890041807283, 'zh': 0.18816591980755518}\n",
      "{'ar': 0.27252025114675166, 'en': 0.8636570619039186, 'es': 0.715216148358068, 'eu': 0.3809991890497137, 'fr': 0.7060198003754777, 'hi': 0.2745921457840218, 'mr': 0.32388663967611336, 'pt': 0.6953077012097959, 'ta': 0.16547788873038516, 'ur': 0.107542284171671, 'vi': 0.4706866024001574, 'zh': 0.14873612444313175}\n",
      "{'ar': 0.27252025114675166, 'en': 0.8636570619039186, 'es': 0.715216148358068, 'eu': 0.3809991890497137, 'fr': 0.7060198003754777, 'hi': 0.2745921457840218, 'mr': 0.32388663967611336, 'pt': 0.6953077012097959, 'ta': 0.16547788873038516, 'ur': 0.107542284171671, 'vi': 0.4706866024001574, 'zh': 0.14873612444313175}\n",
      "{'best': 48.64196131625023, '1000': 30.95094862249302, '10000': 30.95094862249302, '50000': 41.03183633468625, '100000': 40.591180655950666, '150000': 44.00413009499986, '200000': 41.33608046558985, '250000': 42.70534864374338, '300000': 42.70534864374338}\n",
      "defaultdict(<class 'dict'>, {'best': {'ar': 0.4457881244011705, 'en': 0.8548907473714842, 'es': 0.7098428545706285, 'eu': 0.36745447905253825, 'fr': 0.6881672447482472, 'hi': 0.3109705343765607, 'mr': 0.35336976320582875, 'pt': 0.6822043321620779, 'ta': 0.3291139240506329, 'ur': 0.26611547721800755, 'vi': 0.4597988692650223, 'zh': 0.36931900752782976}, '1000': {'ar': 0.20221559916774448, 'en': 0.8185548413323509, 'es': 0.30240275347270124, 'eu': 0.31663388883138394, 'fr': 0.29023683301545344, 'hi': 0.16683689847375024, 'mr': 0.3375, 'pt': 0.3567648256208441, 'ta': 0.21630615640599002, 'ur': 0.10973881352500506, 'vi': 0.30276564774381365, 'zh': 0.2941575771101254}, '10000': {'ar': 0.20221559916774448, 'en': 0.8185548413323509, 'es': 0.30240275347270124, 'eu': 0.31663388883138394, 'fr': 0.29023683301545344, 'hi': 0.16683689847375024, 'mr': 0.3375, 'pt': 0.3567648256208441, 'ta': 0.21630615640599002, 'ur': 0.10973881352500506, 'vi': 0.30276564774381365, 'zh': 0.2941575771101254}, '50000': {'ar': 0.27997624574852886, 'en': 0.8615588848852644, 'es': 0.7141992763913414, 'eu': 0.3600314938086035, 'fr': 0.6993381043577596, 'hi': 0.22057793802581388, 'mr': 0.21875, 'pt': 0.6776955910814179, 'ta': 0.12916666666666665, 'ur': 0.1452228069328441, 'vi': 0.4155240079461214, 'zh': 0.20177934431798877}, '100000': {'ar': 0.22555688942243565, 'en': 0.8633667623588329, 'es': 0.711191605362411, 'eu': 0.3611718408395278, 'fr': 0.7146929116919436, 'hi': 0.2115011604309852, 'mr': 0.2495726495726496, 'pt': 0.6880511091140434, 'ta': 0.17717206132879046, 'ur': 0.13384935992855015, 'vi': 0.4361675817381412, 'zh': 0.09864774692576905}, '150000': {'ar': 0.27279435016338144, 'en': 0.8600099907048506, 'es': 0.7205908935082744, 'eu': 0.3737298517860146, 'fr': 0.7254749069007704, 'hi': 0.2678132429561435, 'mr': 0.323679727427598, 'pt': 0.6880255455029272, 'ta': 0.2106490478041197, 'ur': 0.1443433029908973, 'vi': 0.4570058239122987, 'zh': 0.23637892774270688}, '200000': {'ar': 0.18504734883378163, 'en': 0.8595506684297437, 'es': 0.7245445995445997, 'eu': 0.3806770268642332, 'fr': 0.7092537715517241, 'hi': 0.20405691795337574, 'mr': 0.22137404580152673, 'pt': 0.6845653040435802, 'ta': 0.22240000000000001, 'ur': 0.11960504885993486, 'vi': 0.4610890041807283, 'zh': 0.18816591980755518}, '250000': {'ar': 0.27252025114675166, 'en': 0.8636570619039186, 'es': 0.715216148358068, 'eu': 0.3809991890497137, 'fr': 0.7060198003754777, 'hi': 0.2745921457840218, 'mr': 0.32388663967611336, 'pt': 0.6953077012097959, 'ta': 0.16547788873038516, 'ur': 0.107542284171671, 'vi': 0.4706866024001574, 'zh': 0.14873612444313175}, '300000': {'ar': 0.27252025114675166, 'en': 0.8636570619039186, 'es': 0.715216148358068, 'eu': 0.3809991890497137, 'fr': 0.7060198003754777, 'hi': 0.2745921457840218, 'mr': 0.32388663967611336, 'pt': 0.6953077012097959, 'ta': 0.16547788873038516, 'ur': 0.107542284171671, 'vi': 0.4706866024001574, 'zh': 0.14873612444313175}})\n"
     ]
    }
   ],
   "source": [
    "# ======================read f1 scores==============================\n",
    "# Dict{checkpoint: avg_f1_score}\n",
    "avg_f1_dict = {}\n",
    "# Dict{ckpt: Dict{lang: f1_score}}\n",
    "ckpt_lan_f1_dict: Dict[str, Dict[str, float]] = defaultdict(dict)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5) )\n",
    "\n",
    "for ckpt in checkpoints:\n",
    "    f1_dict = {}\n",
    "    if ckpt == 'best':\n",
    "        output_file_name = os.path.join(output_file_path, model + output_file_suffix, 'test_results.txt')\n",
    "    else:\n",
    "        ckpt_affix = '-intermediate-global_step' + ckpt\n",
    "        output_file_name = os.path.join(output_file_path, model + ckpt_affix + output_file_suffix, 'test_results.txt')\n",
    "    with open(output_file_name, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if 'language' in line:\n",
    "                lang = line.split('=')[1].split('\\n')[0]\n",
    "                f1_score = float(f.readline().split(' = ')[1])\n",
    "                if lang in langs:\n",
    "                    f1_dict[lang] = f1_score \n",
    "        print(f1_dict)\n",
    "        ckpt_lan_f1_dict[ckpt] = f1_dict\n",
    "        avg_f1_score = sum(f1_dict.values())/len(f1_dict.values()) * 100\n",
    "        avg_f1_dict[ckpt] = avg_f1_score\n",
    "\n",
    "print(avg_f1_dict)\n",
    "print(ckpt_lan_f1_dict)\n",
    "# plotting\n",
    "# ckpts, f1_scores = zip(*avg_f1_dict.items())\n",
    "# ax1.set_xlabel('global steps')\n",
    "# ax1.set_ylabel('F1 scores', color='r')\n",
    "# ax1.tick_params(axis='y', labelcolor='r', grid_alpha=0.5)\n",
    "# ax1.plot(ckpts, f1_scores, 'r-')                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop over checkpoints\n",
      "['ara', 'cat', 'fra', 'hin', 'mar', 'por', 'spa', 'tam']\n",
      "[[1.   0.14 0.14 0.16 0.06 0.18 0.18 0.02]\n",
      " [0.14 1.   0.28 0.24 0.08 0.3  0.3  0.04]\n",
      " [0.14 0.28 1.   0.22 0.06 0.42 0.36 0.04]\n",
      " [0.16 0.24 0.22 1.   0.06 0.32 0.3  0.  ]\n",
      " [0.06 0.08 0.06 0.06 1.   0.1  0.14 0.  ]\n",
      " [0.18 0.3  0.42 0.32 0.1  1.   0.46 0.  ]\n",
      " [0.18 0.3  0.36 0.3  0.14 0.46 1.   0.02]\n",
      " [0.02 0.04 0.04 0.   0.   0.   0.02 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'por', 'spa', 'tam', 'urd']\n",
      "[[1.   0.12 0.18 0.04 0.08 0.1  0.14 0.06 0.04 0.06]\n",
      " [0.12 1.   0.32 0.06 0.26 0.1  0.34 0.28 0.08 0.18]\n",
      " [0.18 0.32 1.   0.08 0.2  0.08 0.2  0.2  0.06 0.16]\n",
      " [0.04 0.06 0.08 1.   0.02 0.02 0.08 0.1  0.06 0.06]\n",
      " [0.08 0.26 0.2  0.02 1.   0.12 0.3  0.32 0.12 0.12]\n",
      " [0.1  0.1  0.08 0.02 0.12 1.   0.16 0.1  0.1  0.16]\n",
      " [0.14 0.34 0.2  0.08 0.3  0.16 1.   0.3  0.16 0.2 ]\n",
      " [0.06 0.28 0.2  0.1  0.32 0.1  0.3  1.   0.14 0.12]\n",
      " [0.04 0.08 0.06 0.06 0.12 0.1  0.16 0.14 1.   0.14]\n",
      " [0.06 0.18 0.16 0.06 0.12 0.16 0.2  0.12 0.14 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.14 0.1  0.02 0.12 0.12 0.08 0.12 0.14 0.08 0.08 0.04 0.06]\n",
      " [0.14 1.   0.14 0.04 0.2  0.1  0.02 0.34 0.22 0.04 0.12 0.1  0.1 ]\n",
      " [0.1  0.14 1.   0.02 0.08 0.12 0.14 0.18 0.12 0.08 0.12 0.08 0.14]\n",
      " [0.02 0.04 0.02 1.   0.06 0.12 0.   0.06 0.06 0.08 0.1  0.04 0.02]\n",
      " [0.12 0.2  0.08 0.06 1.   0.14 0.04 0.18 0.2  0.08 0.1  0.1  0.14]\n",
      " [0.12 0.1  0.12 0.12 0.14 1.   0.04 0.06 0.14 0.12 0.14 0.06 0.12]\n",
      " [0.08 0.02 0.14 0.   0.04 0.04 1.   0.06 0.1  0.04 0.02 0.02 0.06]\n",
      " [0.12 0.34 0.18 0.06 0.18 0.06 0.06 1.   0.28 0.06 0.1  0.1  0.06]\n",
      " [0.14 0.22 0.12 0.06 0.2  0.14 0.1  0.28 1.   0.08 0.1  0.1  0.1 ]\n",
      " [0.08 0.04 0.08 0.08 0.08 0.12 0.04 0.06 0.08 1.   0.08 0.08 0.06]\n",
      " [0.08 0.12 0.12 0.1  0.1  0.14 0.02 0.1  0.1  0.08 1.   0.06 0.14]\n",
      " [0.04 0.1  0.08 0.04 0.1  0.06 0.02 0.1  0.1  0.08 0.06 1.   0.08]\n",
      " [0.06 0.1  0.14 0.02 0.14 0.12 0.06 0.06 0.1  0.06 0.14 0.08 1.  ]]\n",
      "[0.1, 0.14, 1.0, 0.02, 0.08, 0.12, 0.14, 0.18, 0.12, 0.08, 0.12, 0.08, 0.14]\n",
      "<class 'list'>\n",
      "['ara', 'cat', 'fra', 'hin', 'mar', 'por', 'spa', 'tam']\n",
      "[[1.   0.08 0.06 0.04 0.02 0.   0.02 0.02]\n",
      " [0.08 1.   0.26 0.04 0.04 0.14 0.18 0.02]\n",
      " [0.06 0.26 1.   0.02 0.02 0.1  0.18 0.04]\n",
      " [0.04 0.04 0.02 1.   0.02 0.   0.   0.  ]\n",
      " [0.02 0.04 0.02 0.02 1.   0.06 0.04 0.02]\n",
      " [0.   0.14 0.1  0.   0.06 1.   0.26 0.02]\n",
      " [0.02 0.18 0.18 0.   0.04 0.26 1.   0.  ]\n",
      " [0.02 0.02 0.04 0.   0.02 0.02 0.   1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'por', 'spa', 'tam', 'urd']\n",
      "[[1.   0.   0.04 0.   0.02 0.02 0.06 0.04 0.02 0.02]\n",
      " [0.   1.   0.12 0.   0.1  0.02 0.2  0.12 0.06 0.02]\n",
      " [0.04 0.12 1.   0.   0.1  0.   0.06 0.08 0.1  0.04]\n",
      " [0.   0.   0.   1.   0.02 0.04 0.04 0.02 0.02 0.08]\n",
      " [0.02 0.1  0.1  0.02 1.   0.02 0.1  0.1  0.02 0.  ]\n",
      " [0.02 0.02 0.   0.04 0.02 1.   0.   0.02 0.02 0.04]\n",
      " [0.06 0.2  0.06 0.04 0.1  0.   1.   0.22 0.02 0.  ]\n",
      " [0.04 0.12 0.08 0.02 0.1  0.02 0.22 1.   0.   0.02]\n",
      " [0.02 0.06 0.1  0.02 0.02 0.02 0.02 0.   1.   0.04]\n",
      " [0.02 0.02 0.04 0.08 0.   0.04 0.   0.02 0.04 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.02 0.   0.   0.04 0.02 0.   0.04 0.04 0.04 0.06 0.   0.04]\n",
      " [0.02 1.   0.08 0.08 0.06 0.   0.02 0.1  0.06 0.   0.04 0.04 0.02]\n",
      " [0.   0.08 1.   0.   0.04 0.   0.02 0.04 0.02 0.   0.02 0.08 0.  ]\n",
      " [0.   0.08 0.   1.   0.   0.02 0.02 0.02 0.02 0.   0.   0.04 0.04]\n",
      " [0.04 0.06 0.04 0.   1.   0.   0.02 0.02 0.08 0.02 0.02 0.02 0.04]\n",
      " [0.02 0.   0.   0.02 0.   1.   0.04 0.   0.06 0.   0.04 0.02 0.  ]\n",
      " [0.   0.02 0.02 0.02 0.02 0.04 1.   0.08 0.02 0.02 0.02 0.   0.02]\n",
      " [0.04 0.1  0.04 0.02 0.02 0.   0.08 1.   0.   0.   0.08 0.04 0.04]\n",
      " [0.04 0.06 0.02 0.02 0.08 0.06 0.02 0.   1.   0.06 0.04 0.04 0.02]\n",
      " [0.04 0.   0.   0.   0.02 0.   0.02 0.   0.06 1.   0.08 0.02 0.04]\n",
      " [0.06 0.04 0.02 0.   0.02 0.04 0.02 0.08 0.04 0.08 1.   0.   0.02]\n",
      " [0.   0.04 0.08 0.04 0.02 0.02 0.   0.04 0.04 0.02 0.   1.   0.08]\n",
      " [0.04 0.02 0.   0.04 0.04 0.   0.02 0.04 0.02 0.04 0.02 0.08 1.  ]]\n",
      "[0.0, 0.08, 1.0, 0.0, 0.04, 0.0, 0.02, 0.04, 0.02, 0.0, 0.02, 0.08, 0.0]\n",
      "<class 'list'>\n",
      "['ara', 'cat', 'fra', 'hin', 'mar', 'por', 'spa', 'tam']\n",
      "[[1.   0.   0.06 0.04 0.04 0.04 0.02 0.04]\n",
      " [0.   1.   0.14 0.04 0.02 0.12 0.18 0.02]\n",
      " [0.06 0.14 1.   0.06 0.02 0.1  0.18 0.04]\n",
      " [0.04 0.04 0.06 1.   0.02 0.02 0.04 0.  ]\n",
      " [0.04 0.02 0.02 0.02 1.   0.02 0.08 0.04]\n",
      " [0.04 0.12 0.1  0.02 0.02 1.   0.2  0.04]\n",
      " [0.02 0.18 0.18 0.04 0.08 0.2  1.   0.04]\n",
      " [0.04 0.02 0.04 0.   0.04 0.04 0.04 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'por', 'spa', 'tam', 'urd']\n",
      "[[1.   0.02 0.02 0.02 0.04 0.02 0.04 0.04 0.06 0.02]\n",
      " [0.02 1.   0.08 0.02 0.16 0.04 0.18 0.14 0.   0.06]\n",
      " [0.02 0.08 1.   0.02 0.12 0.   0.02 0.04 0.1  0.  ]\n",
      " [0.02 0.02 0.02 1.   0.02 0.   0.1  0.06 0.04 0.02]\n",
      " [0.04 0.16 0.12 0.02 1.   0.06 0.14 0.16 0.04 0.  ]\n",
      " [0.02 0.04 0.   0.   0.06 1.   0.04 0.04 0.   0.02]\n",
      " [0.04 0.18 0.02 0.1  0.14 0.04 1.   0.24 0.02 0.02]\n",
      " [0.04 0.14 0.04 0.06 0.16 0.04 0.24 1.   0.04 0.  ]\n",
      " [0.06 0.   0.1  0.04 0.04 0.   0.02 0.04 1.   0.02]\n",
      " [0.02 0.06 0.   0.02 0.   0.02 0.02 0.   0.02 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.   0.   0.   0.06 0.02 0.04 0.06 0.   0.02 0.04 0.04 0.02]\n",
      " [0.   1.   0.04 0.04 0.06 0.06 0.02 0.04 0.   0.02 0.02 0.   0.04]\n",
      " [0.   0.04 1.   0.   0.04 0.   0.   0.02 0.1  0.04 0.   0.1  0.04]\n",
      " [0.   0.04 0.   1.   0.12 0.02 0.04 0.04 0.   0.02 0.02 0.06 0.06]\n",
      " [0.06 0.06 0.04 0.12 1.   0.02 0.02 0.02 0.04 0.02 0.   0.   0.04]\n",
      " [0.02 0.06 0.   0.02 0.02 1.   0.04 0.04 0.02 0.   0.   0.04 0.04]\n",
      " [0.04 0.02 0.   0.04 0.02 0.04 1.   0.08 0.04 0.02 0.02 0.04 0.04]\n",
      " [0.06 0.04 0.02 0.04 0.02 0.04 0.08 1.   0.08 0.02 0.08 0.02 0.04]\n",
      " [0.   0.   0.1  0.   0.04 0.02 0.04 0.08 1.   0.02 0.   0.04 0.02]\n",
      " [0.02 0.02 0.04 0.02 0.02 0.   0.02 0.02 0.02 1.   0.06 0.04 0.02]\n",
      " [0.04 0.02 0.   0.02 0.   0.   0.02 0.08 0.   0.06 1.   0.02 0.06]\n",
      " [0.04 0.   0.1  0.06 0.   0.04 0.04 0.02 0.04 0.04 0.02 1.   0.  ]\n",
      " [0.02 0.04 0.04 0.06 0.04 0.04 0.04 0.04 0.02 0.02 0.06 0.   1.  ]]\n",
      "[0.0, 0.04, 1.0, 0.0, 0.04, 0.0, 0.0, 0.02, 0.1, 0.04, 0.0, 0.1, 0.04]\n",
      "<class 'list'>\n",
      "['ara', 'cat', 'fra', 'hin', 'mar', 'por', 'spa', 'tam']\n",
      "[[1.   0.04 0.1  0.04 0.02 0.   0.02 0.02]\n",
      " [0.04 1.   0.1  0.04 0.02 0.04 0.22 0.  ]\n",
      " [0.1  0.1  1.   0.08 0.04 0.3  0.2  0.02]\n",
      " [0.04 0.04 0.08 1.   0.04 0.04 0.08 0.04]\n",
      " [0.02 0.02 0.04 0.04 1.   0.02 0.02 0.04]\n",
      " [0.   0.04 0.3  0.04 0.02 1.   0.28 0.02]\n",
      " [0.02 0.22 0.2  0.08 0.02 0.28 1.   0.  ]\n",
      " [0.02 0.   0.02 0.04 0.04 0.02 0.   1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'por', 'spa', 'tam', 'urd']\n",
      "[[1.   0.06 0.06 0.08 0.06 0.1  0.02 0.04 0.06 0.04]\n",
      " [0.06 1.   0.04 0.02 0.24 0.08 0.12 0.12 0.02 0.02]\n",
      " [0.06 0.04 1.   0.1  0.04 0.1  0.06 0.04 0.04 0.14]\n",
      " [0.08 0.02 0.1  1.   0.   0.08 0.02 0.   0.04 0.08]\n",
      " [0.06 0.24 0.04 0.   1.   0.06 0.18 0.18 0.02 0.04]\n",
      " [0.1  0.08 0.1  0.08 0.06 1.   0.12 0.08 0.02 0.16]\n",
      " [0.02 0.12 0.06 0.02 0.18 0.12 1.   0.26 0.04 0.06]\n",
      " [0.04 0.12 0.04 0.   0.18 0.08 0.26 1.   0.02 0.04]\n",
      " [0.06 0.02 0.04 0.04 0.02 0.02 0.04 0.02 1.   0.06]\n",
      " [0.04 0.02 0.14 0.08 0.04 0.16 0.06 0.04 0.06 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.08 0.08 0.   0.08 0.06 0.   0.1  0.04 0.   0.   0.   0.02]\n",
      " [0.08 1.   0.04 0.02 0.1  0.06 0.   0.16 0.06 0.   0.02 0.02 0.  ]\n",
      " [0.08 0.04 1.   0.   0.1  0.04 0.04 0.   0.08 0.02 0.04 0.04 0.06]\n",
      " [0.   0.02 0.   1.   0.02 0.02 0.06 0.02 0.06 0.04 0.02 0.04 0.08]\n",
      " [0.08 0.1  0.1  0.02 1.   0.12 0.02 0.16 0.06 0.02 0.02 0.08 0.04]\n",
      " [0.06 0.06 0.04 0.02 0.12 1.   0.   0.04 0.08 0.02 0.   0.02 0.  ]\n",
      " [0.   0.   0.04 0.06 0.02 0.   1.   0.02 0.04 0.06 0.   0.04 0.02]\n",
      " [0.1  0.16 0.   0.02 0.16 0.04 0.02 1.   0.08 0.04 0.02 0.04 0.06]\n",
      " [0.04 0.06 0.08 0.06 0.06 0.08 0.04 0.08 1.   0.02 0.   0.04 0.02]\n",
      " [0.   0.   0.02 0.04 0.02 0.02 0.06 0.04 0.02 1.   0.02 0.04 0.06]\n",
      " [0.   0.02 0.04 0.02 0.02 0.   0.   0.02 0.   0.02 1.   0.04 0.02]\n",
      " [0.   0.02 0.04 0.04 0.08 0.02 0.04 0.04 0.04 0.04 0.04 1.   0.1 ]\n",
      " [0.02 0.   0.06 0.08 0.04 0.   0.02 0.06 0.02 0.06 0.02 0.1  1.  ]]\n",
      "[0.08, 0.04, 1.0, 0.0, 0.1, 0.04, 0.04, 0.0, 0.08, 0.02, 0.04, 0.04, 0.06]\n",
      "<class 'list'>\n",
      "['ara', 'cat', 'fra', 'hin', 'mar', 'por', 'spa', 'tam']\n",
      "[[1.   0.06 0.02 0.02 0.04 0.04 0.06 0.  ]\n",
      " [0.06 1.   0.18 0.06 0.   0.28 0.18 0.04]\n",
      " [0.02 0.18 1.   0.08 0.   0.36 0.22 0.02]\n",
      " [0.02 0.06 0.08 1.   0.06 0.04 0.04 0.02]\n",
      " [0.04 0.   0.   0.06 1.   0.02 0.04 0.02]\n",
      " [0.04 0.28 0.36 0.04 0.02 1.   0.18 0.06]\n",
      " [0.06 0.18 0.22 0.04 0.04 0.18 1.   0.04]\n",
      " [0.   0.04 0.02 0.02 0.02 0.06 0.04 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'por', 'spa', 'tam', 'urd']\n",
      "[[1.   0.08 0.06 0.04 0.06 0.1  0.06 0.06 0.04 0.08]\n",
      " [0.08 1.   0.14 0.06 0.14 0.08 0.2  0.24 0.08 0.06]\n",
      " [0.06 0.14 1.   0.12 0.08 0.18 0.12 0.16 0.1  0.1 ]\n",
      " [0.04 0.06 0.12 1.   0.04 0.08 0.02 0.1  0.06 0.1 ]\n",
      " [0.06 0.14 0.08 0.04 1.   0.04 0.22 0.2  0.04 0.06]\n",
      " [0.1  0.08 0.18 0.08 0.04 1.   0.16 0.1  0.1  0.12]\n",
      " [0.06 0.2  0.12 0.02 0.22 0.16 1.   0.26 0.1  0.06]\n",
      " [0.06 0.24 0.16 0.1  0.2  0.1  0.26 1.   0.04 0.08]\n",
      " [0.04 0.08 0.1  0.06 0.04 0.1  0.1  0.04 1.   0.12]\n",
      " [0.08 0.06 0.1  0.1  0.06 0.12 0.06 0.08 0.12 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.06 0.04 0.02 0.08 0.08 0.   0.1  0.02 0.06 0.   0.04 0.06]\n",
      " [0.06 1.   0.08 0.02 0.1  0.02 0.02 0.1  0.1  0.04 0.02 0.04 0.04]\n",
      " [0.04 0.08 1.   0.02 0.1  0.06 0.02 0.08 0.18 0.04 0.1  0.04 0.04]\n",
      " [0.02 0.02 0.02 1.   0.04 0.02 0.08 0.02 0.02 0.02 0.08 0.02 0.  ]\n",
      " [0.08 0.1  0.1  0.04 1.   0.04 0.02 0.16 0.1  0.06 0.1  0.02 0.1 ]\n",
      " [0.08 0.02 0.06 0.02 0.04 1.   0.04 0.06 0.02 0.08 0.06 0.   0.06]\n",
      " [0.   0.02 0.02 0.08 0.02 0.04 1.   0.   0.02 0.02 0.02 0.   0.02]\n",
      " [0.1  0.1  0.08 0.02 0.16 0.06 0.   1.   0.2  0.   0.04 0.1  0.06]\n",
      " [0.02 0.1  0.18 0.02 0.1  0.02 0.02 0.2  1.   0.04 0.1  0.06 0.06]\n",
      " [0.06 0.04 0.04 0.02 0.06 0.08 0.02 0.   0.04 1.   0.08 0.   0.02]\n",
      " [0.   0.02 0.1  0.08 0.1  0.06 0.02 0.04 0.1  0.08 1.   0.02 0.04]\n",
      " [0.04 0.04 0.04 0.02 0.02 0.   0.   0.1  0.06 0.   0.02 1.   0.02]\n",
      " [0.06 0.04 0.04 0.   0.1  0.06 0.02 0.06 0.06 0.02 0.04 0.02 1.  ]]\n",
      "[0.04, 0.08, 1.0, 0.02, 0.1, 0.06, 0.02, 0.08, 0.18, 0.04, 0.1, 0.04, 0.04]\n",
      "<class 'list'>\n",
      "['ara', 'cat', 'fra', 'hin', 'mar', 'por', 'spa', 'tam']\n",
      "[[1.   0.1  0.08 0.   0.02 0.08 0.1  0.04]\n",
      " [0.1  1.   0.1  0.06 0.04 0.26 0.18 0.06]\n",
      " [0.08 0.1  1.   0.02 0.   0.3  0.24 0.02]\n",
      " [0.   0.06 0.02 1.   0.08 0.02 0.12 0.  ]\n",
      " [0.02 0.04 0.   0.08 1.   0.02 0.04 0.04]\n",
      " [0.08 0.26 0.3  0.02 0.02 1.   0.24 0.06]\n",
      " [0.1  0.18 0.24 0.12 0.04 0.24 1.   0.08]\n",
      " [0.04 0.06 0.02 0.   0.04 0.06 0.08 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'por', 'spa', 'tam', 'urd']\n",
      "[[1.   0.16 0.12 0.08 0.04 0.18 0.12 0.12 0.1  0.08]\n",
      " [0.16 1.   0.16 0.06 0.22 0.18 0.24 0.28 0.04 0.08]\n",
      " [0.12 0.16 1.   0.08 0.02 0.22 0.16 0.14 0.06 0.14]\n",
      " [0.08 0.06 0.08 1.   0.02 0.12 0.06 0.08 0.14 0.08]\n",
      " [0.04 0.22 0.02 0.02 1.   0.06 0.16 0.12 0.08 0.06]\n",
      " [0.18 0.18 0.22 0.12 0.06 1.   0.2  0.18 0.12 0.28]\n",
      " [0.12 0.24 0.16 0.06 0.16 0.2  1.   0.28 0.08 0.12]\n",
      " [0.12 0.28 0.14 0.08 0.12 0.18 0.28 1.   0.04 0.1 ]\n",
      " [0.1  0.04 0.06 0.14 0.08 0.12 0.08 0.04 1.   0.08]\n",
      " [0.08 0.08 0.14 0.08 0.06 0.28 0.12 0.1  0.08 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.08 0.02 0.02 0.04 0.04 0.   0.1  0.02 0.   0.02 0.06 0.02]\n",
      " [0.08 1.   0.1  0.06 0.1  0.06 0.   0.12 0.16 0.06 0.06 0.02 0.06]\n",
      " [0.02 0.1  1.   0.06 0.1  0.04 0.   0.04 0.06 0.02 0.1  0.04 0.06]\n",
      " [0.02 0.06 0.06 1.   0.04 0.04 0.   0.   0.04 0.06 0.02 0.08 0.06]\n",
      " [0.04 0.1  0.1  0.04 1.   0.04 0.02 0.16 0.18 0.02 0.04 0.08 0.12]\n",
      " [0.04 0.06 0.04 0.04 0.04 1.   0.02 0.04 0.1  0.04 0.14 0.02 0.08]\n",
      " [0.   0.   0.   0.   0.02 0.02 1.   0.02 0.   0.   0.02 0.02 0.04]\n",
      " [0.1  0.12 0.04 0.   0.16 0.04 0.02 1.   0.16 0.   0.1  0.04 0.06]\n",
      " [0.02 0.16 0.06 0.04 0.18 0.1  0.   0.16 1.   0.06 0.08 0.06 0.1 ]\n",
      " [0.   0.06 0.02 0.06 0.02 0.04 0.   0.   0.06 1.   0.02 0.   0.06]\n",
      " [0.02 0.06 0.1  0.02 0.04 0.14 0.02 0.1  0.08 0.02 1.   0.04 0.06]\n",
      " [0.06 0.02 0.04 0.08 0.08 0.02 0.02 0.04 0.06 0.   0.04 1.   0.08]\n",
      " [0.02 0.06 0.06 0.06 0.12 0.08 0.04 0.06 0.1  0.06 0.06 0.08 1.  ]]\n",
      "[0.02, 0.1, 1.0, 0.06, 0.1, 0.04, 0.0, 0.04, 0.06, 0.02, 0.1, 0.04, 0.06]\n",
      "<class 'list'>\n",
      "['ara', 'cat', 'fra', 'hin', 'mar', 'por', 'spa', 'tam']\n",
      "[[1.   0.08 0.06 0.06 0.02 0.1  0.1  0.04]\n",
      " [0.08 1.   0.2  0.04 0.04 0.18 0.22 0.  ]\n",
      " [0.06 0.2  1.   0.12 0.06 0.28 0.22 0.02]\n",
      " [0.06 0.04 0.12 1.   0.08 0.12 0.16 0.02]\n",
      " [0.02 0.04 0.06 0.08 1.   0.02 0.02 0.08]\n",
      " [0.1  0.18 0.28 0.12 0.02 1.   0.36 0.08]\n",
      " [0.1  0.22 0.22 0.16 0.02 0.36 1.   0.08]\n",
      " [0.04 0.   0.02 0.02 0.08 0.08 0.08 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'por', 'spa', 'tam', 'urd']\n",
      "[[1.   0.12 0.1  0.06 0.06 0.06 0.1  0.12 0.04 0.06]\n",
      " [0.12 1.   0.18 0.08 0.26 0.12 0.2  0.2  0.12 0.1 ]\n",
      " [0.1  0.18 1.   0.1  0.08 0.1  0.12 0.14 0.14 0.08]\n",
      " [0.06 0.08 0.1  1.   0.   0.06 0.02 0.08 0.06 0.08]\n",
      " [0.06 0.26 0.08 0.   1.   0.08 0.24 0.24 0.08 0.1 ]\n",
      " [0.06 0.12 0.1  0.06 0.08 1.   0.16 0.16 0.12 0.18]\n",
      " [0.1  0.2  0.12 0.02 0.24 0.16 1.   0.28 0.06 0.08]\n",
      " [0.12 0.2  0.14 0.08 0.24 0.16 0.28 1.   0.1  0.12]\n",
      " [0.04 0.12 0.14 0.06 0.08 0.12 0.06 0.1  1.   0.16]\n",
      " [0.06 0.1  0.08 0.08 0.1  0.18 0.08 0.12 0.16 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.2  0.04 0.02 0.12 0.12 0.08 0.06 0.06 0.06 0.04 0.06 0.02]\n",
      " [0.2  1.   0.1  0.02 0.18 0.08 0.02 0.14 0.16 0.02 0.06 0.   0.06]\n",
      " [0.04 0.1  1.   0.04 0.14 0.06 0.02 0.1  0.14 0.02 0.04 0.1  0.04]\n",
      " [0.02 0.02 0.04 1.   0.04 0.08 0.06 0.04 0.02 0.1  0.08 0.   0.08]\n",
      " [0.12 0.18 0.14 0.04 1.   0.06 0.   0.16 0.16 0.06 0.08 0.06 0.06]\n",
      " [0.12 0.08 0.06 0.08 0.06 1.   0.02 0.06 0.04 0.04 0.04 0.04 0.08]\n",
      " [0.08 0.02 0.02 0.06 0.   0.02 1.   0.06 0.02 0.04 0.06 0.02 0.  ]\n",
      " [0.06 0.14 0.1  0.04 0.16 0.06 0.06 1.   0.2  0.04 0.08 0.1  0.04]\n",
      " [0.06 0.16 0.14 0.02 0.16 0.04 0.02 0.2  1.   0.06 0.12 0.02 0.08]\n",
      " [0.06 0.02 0.02 0.1  0.06 0.04 0.04 0.04 0.06 1.   0.1  0.04 0.04]\n",
      " [0.04 0.06 0.04 0.08 0.08 0.04 0.06 0.08 0.12 0.1  1.   0.02 0.12]\n",
      " [0.06 0.   0.1  0.   0.06 0.04 0.02 0.1  0.02 0.04 0.02 1.   0.04]\n",
      " [0.02 0.06 0.04 0.08 0.06 0.08 0.   0.04 0.08 0.04 0.12 0.04 1.  ]]\n",
      "[0.04, 0.1, 1.0, 0.04, 0.14, 0.06, 0.02, 0.1, 0.14, 0.02, 0.04, 0.1, 0.04]\n",
      "<class 'list'>\n",
      "['ara', 'cat', 'fra', 'hin', 'mar', 'por', 'spa', 'tam']\n",
      "[[1.   0.18 0.12 0.02 0.02 0.1  0.2  0.04]\n",
      " [0.18 1.   0.26 0.08 0.08 0.34 0.24 0.04]\n",
      " [0.12 0.26 1.   0.18 0.06 0.34 0.3  0.02]\n",
      " [0.02 0.08 0.18 1.   0.08 0.16 0.24 0.02]\n",
      " [0.02 0.08 0.06 0.08 1.   0.08 0.06 0.04]\n",
      " [0.1  0.34 0.34 0.16 0.08 1.   0.32 0.02]\n",
      " [0.2  0.24 0.3  0.24 0.06 0.32 1.   0.02]\n",
      " [0.04 0.04 0.02 0.02 0.04 0.02 0.02 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'por', 'spa', 'tam', 'urd']\n",
      "[[1.   0.14 0.08 0.04 0.06 0.1  0.04 0.08 0.04 0.06]\n",
      " [0.14 1.   0.14 0.08 0.2  0.06 0.22 0.28 0.08 0.1 ]\n",
      " [0.08 0.14 1.   0.06 0.16 0.2  0.18 0.22 0.16 0.14]\n",
      " [0.04 0.08 0.06 1.   0.   0.02 0.02 0.08 0.06 0.08]\n",
      " [0.06 0.2  0.16 0.   1.   0.1  0.28 0.28 0.04 0.06]\n",
      " [0.1  0.06 0.2  0.02 0.1  1.   0.08 0.12 0.14 0.18]\n",
      " [0.04 0.22 0.18 0.02 0.28 0.08 1.   0.24 0.14 0.1 ]\n",
      " [0.08 0.28 0.22 0.08 0.28 0.12 0.24 1.   0.14 0.1 ]\n",
      " [0.04 0.08 0.16 0.06 0.04 0.14 0.14 0.14 1.   0.14]\n",
      " [0.06 0.1  0.14 0.08 0.06 0.18 0.1  0.1  0.14 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.1  0.08 0.04 0.1  0.12 0.06 0.1  0.08 0.04 0.04 0.08 0.08]\n",
      " [0.1  1.   0.08 0.04 0.26 0.04 0.02 0.14 0.16 0.04 0.06 0.04 0.08]\n",
      " [0.08 0.08 1.   0.   0.16 0.16 0.08 0.14 0.1  0.08 0.06 0.08 0.06]\n",
      " [0.04 0.04 0.   1.   0.06 0.04 0.04 0.02 0.04 0.04 0.02 0.06 0.  ]\n",
      " [0.1  0.26 0.16 0.06 1.   0.1  0.06 0.18 0.22 0.08 0.04 0.1  0.08]\n",
      " [0.12 0.04 0.16 0.04 0.1  1.   0.04 0.08 0.12 0.18 0.1  0.1  0.12]\n",
      " [0.06 0.02 0.08 0.04 0.06 0.04 1.   0.   0.1  0.04 0.06 0.02 0.  ]\n",
      " [0.1  0.14 0.14 0.02 0.18 0.08 0.   1.   0.18 0.1  0.02 0.12 0.08]\n",
      " [0.08 0.16 0.1  0.04 0.22 0.12 0.1  0.18 1.   0.06 0.04 0.06 0.1 ]\n",
      " [0.04 0.04 0.08 0.04 0.08 0.18 0.04 0.1  0.06 1.   0.14 0.08 0.08]\n",
      " [0.04 0.06 0.06 0.02 0.04 0.1  0.06 0.02 0.04 0.14 1.   0.02 0.06]\n",
      " [0.08 0.04 0.08 0.06 0.1  0.1  0.02 0.12 0.06 0.08 0.02 1.   0.04]\n",
      " [0.08 0.08 0.06 0.   0.08 0.12 0.   0.08 0.1  0.08 0.06 0.04 1.  ]]\n",
      "[0.08, 0.08, 1.0, 0.0, 0.16, 0.16, 0.08, 0.14, 0.1, 0.08, 0.06, 0.08, 0.06]\n",
      "<class 'list'>\n",
      "['ara', 'cat', 'fra', 'hin', 'mar', 'por', 'spa', 'tam']\n",
      "[[1.   0.16 0.1  0.08 0.04 0.14 0.14 0.  ]\n",
      " [0.16 1.   0.24 0.16 0.08 0.32 0.32 0.02]\n",
      " [0.1  0.24 1.   0.16 0.1  0.38 0.24 0.  ]\n",
      " [0.08 0.16 0.16 1.   0.06 0.18 0.22 0.  ]\n",
      " [0.04 0.08 0.1  0.06 1.   0.06 0.06 0.  ]\n",
      " [0.14 0.32 0.38 0.18 0.06 1.   0.36 0.  ]\n",
      " [0.14 0.32 0.24 0.22 0.06 0.36 1.   0.  ]\n",
      " [0.   0.02 0.   0.   0.   0.   0.   1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'por', 'spa', 'tam', 'urd']\n",
      "[[1.   0.1  0.08 0.04 0.08 0.08 0.06 0.06 0.04 0.04]\n",
      " [0.1  1.   0.16 0.06 0.32 0.08 0.32 0.26 0.04 0.14]\n",
      " [0.08 0.16 1.   0.12 0.18 0.12 0.22 0.18 0.14 0.1 ]\n",
      " [0.04 0.06 0.12 1.   0.02 0.06 0.04 0.06 0.08 0.08]\n",
      " [0.08 0.32 0.18 0.02 1.   0.08 0.26 0.24 0.1  0.1 ]\n",
      " [0.08 0.08 0.12 0.06 0.08 1.   0.1  0.08 0.16 0.12]\n",
      " [0.06 0.32 0.22 0.04 0.26 0.1  1.   0.26 0.16 0.14]\n",
      " [0.06 0.26 0.18 0.06 0.24 0.08 0.26 1.   0.1  0.12]\n",
      " [0.04 0.04 0.14 0.08 0.1  0.16 0.16 0.1  1.   0.12]\n",
      " [0.04 0.14 0.1  0.08 0.1  0.12 0.14 0.12 0.12 1.  ]]\n",
      "['ara', 'cat', 'eng', 'eus', 'fra', 'hin', 'mar', 'por', 'spa', 'tam', 'urd', 'vie', 'zho']\n",
      "[[1.   0.1  0.06 0.06 0.14 0.08 0.04 0.14 0.16 0.06 0.04 0.06 0.08]\n",
      " [0.1  1.   0.1  0.1  0.16 0.06 0.02 0.18 0.2  0.06 0.06 0.04 0.1 ]\n",
      " [0.06 0.1  1.   0.04 0.14 0.06 0.04 0.12 0.1  0.06 0.04 0.12 0.1 ]\n",
      " [0.06 0.1  0.04 1.   0.08 0.06 0.04 0.06 0.08 0.08 0.06 0.06 0.02]\n",
      " [0.14 0.16 0.14 0.08 1.   0.1  0.02 0.18 0.14 0.14 0.1  0.12 0.18]\n",
      " [0.08 0.06 0.06 0.06 0.1  1.   0.04 0.1  0.08 0.14 0.08 0.12 0.1 ]\n",
      " [0.04 0.02 0.04 0.04 0.02 0.04 1.   0.02 0.02 0.04 0.06 0.02 0.02]\n",
      " [0.14 0.18 0.12 0.06 0.18 0.1  0.02 1.   0.26 0.12 0.06 0.1  0.1 ]\n",
      " [0.16 0.2  0.1  0.08 0.14 0.08 0.02 0.26 1.   0.06 0.12 0.04 0.1 ]\n",
      " [0.06 0.06 0.06 0.08 0.14 0.14 0.04 0.12 0.06 1.   0.1  0.1  0.1 ]\n",
      " [0.04 0.06 0.04 0.06 0.1  0.08 0.06 0.06 0.12 0.1  1.   0.02 0.06]\n",
      " [0.06 0.04 0.12 0.06 0.12 0.12 0.02 0.1  0.04 0.1  0.02 1.   0.06]\n",
      " [0.08 0.1  0.1  0.02 0.18 0.1  0.02 0.1  0.1  0.1  0.06 0.06 1.  ]]\n",
      "[0.06, 0.1, 1.0, 0.04, 0.14, 0.06, 0.04, 0.12, 0.1, 0.06, 0.04, 0.12, 0.1]\n",
      "<class 'list'>\n",
      "defaultdict(<class 'dict'>, {'Gender': {'best': 0.165, '1000': 0.06071428571428572, '10000': 0.05928571428571429, '50000': 0.06714285714285714, '100000': 0.07785714285714286, '150000': 0.08571428571428572, '200000': 0.10214285714285713, '250000': 0.13071428571428573, '300000': 0.12928571428571428}, 'Number': {'best': 0.14044444444444446, '1000': 0.04622222222222223, '10000': 0.052000000000000005, '50000': 0.07244444444444445, '100000': 0.10088888888888888, '150000': 0.12311111111111112, '200000': 0.11555555555555554, '250000': 0.11822222222222219, '300000': 0.12222222222222222}, 'POS': {'best': 0.09717948717948717, '1000': 0.029230769230769234, '10000': 0.031538461538461536, '50000': 0.04076923076923077, '100000': 0.050769230769230775, '150000': 0.05384615384615385, '200000': 0.06641025641025641, '250000': 0.07846153846153844, '300000': 0.08564102564102562}})\n",
      "defaultdict(<class 'dict'>, {'best': {'ara': 0.1, 'cat': 0.14, 'eng': 1.0, 'eus': 0.02, 'fra': 0.08, 'hin': 0.12, 'mar': 0.14, 'por': 0.18, 'spa': 0.12, 'tam': 0.08, 'urd': 0.12, 'vie': 0.08, 'zho': 0.14}, '1000': {'ara': 0.0, 'cat': 0.08, 'eng': 1.0, 'eus': 0.0, 'fra': 0.04, 'hin': 0.0, 'mar': 0.02, 'por': 0.04, 'spa': 0.02, 'tam': 0.0, 'urd': 0.02, 'vie': 0.08, 'zho': 0.0}, '10000': {'ara': 0.0, 'cat': 0.04, 'eng': 1.0, 'eus': 0.0, 'fra': 0.04, 'hin': 0.0, 'mar': 0.0, 'por': 0.02, 'spa': 0.1, 'tam': 0.04, 'urd': 0.0, 'vie': 0.1, 'zho': 0.04}, '50000': {'ara': 0.08, 'cat': 0.04, 'eng': 1.0, 'eus': 0.0, 'fra': 0.1, 'hin': 0.04, 'mar': 0.04, 'por': 0.0, 'spa': 0.08, 'tam': 0.02, 'urd': 0.04, 'vie': 0.04, 'zho': 0.06}, '100000': {'ara': 0.04, 'cat': 0.08, 'eng': 1.0, 'eus': 0.02, 'fra': 0.1, 'hin': 0.06, 'mar': 0.02, 'por': 0.08, 'spa': 0.18, 'tam': 0.04, 'urd': 0.1, 'vie': 0.04, 'zho': 0.04}, '150000': {'ara': 0.02, 'cat': 0.1, 'eng': 1.0, 'eus': 0.06, 'fra': 0.1, 'hin': 0.04, 'mar': 0.0, 'por': 0.04, 'spa': 0.06, 'tam': 0.02, 'urd': 0.1, 'vie': 0.04, 'zho': 0.06}, '200000': {'ara': 0.04, 'cat': 0.1, 'eng': 1.0, 'eus': 0.04, 'fra': 0.14, 'hin': 0.06, 'mar': 0.02, 'por': 0.1, 'spa': 0.14, 'tam': 0.02, 'urd': 0.04, 'vie': 0.1, 'zho': 0.04}, '250000': {'ara': 0.08, 'cat': 0.08, 'eng': 1.0, 'eus': 0.0, 'fra': 0.16, 'hin': 0.16, 'mar': 0.08, 'por': 0.14, 'spa': 0.1, 'tam': 0.08, 'urd': 0.06, 'vie': 0.08, 'zho': 0.06}, '300000': {'ara': 0.06, 'cat': 0.1, 'eng': 1.0, 'eus': 0.04, 'fra': 0.14, 'hin': 0.06, 'mar': 0.04, 'por': 0.12, 'spa': 0.1, 'tam': 0.06, 'urd': 0.04, 'vie': 0.12, 'zho': 0.1}})\n"
     ]
    }
   ],
   "source": [
    "# ===========================read overlap ratios============================\n",
    "# Dict{attr: Dict{checkpoint: avg_overlap_rate}}\n",
    "avg_overlap_rates: Dict[str, Dict[str, float]] = defaultdict(dict)\n",
    "# Dict{ckpt: Dict{lang: overlap_rate_with_eng}}, this is to record the overlap rate with english only.\n",
    "pos_ovlp_dict: Dict[str, Dict[str, float]] = defaultdict(dict)\n",
    "\n",
    "if layers is not None:\n",
    "    print(\"Loop over layers\")\n",
    "    for layer in layers:\n",
    "        embedding = model\n",
    "        experiment_name = f\"inter-layer-{layer}\"\n",
    "        if layer == 25:\n",
    "            experiment_name = \"last-layer\"\n",
    "        DEFAULT_RESULTS_FOLDER = f\"results/{embedding}/{experiment_name}\"\n",
    "\n",
    "        # print(listdir(DEFAULT_RESULTS_FOLDER))\n",
    "        RESULTS = []\n",
    "        rel_treebanks = []\n",
    "        for lang in listdir(DEFAULT_RESULTS_FOLDER):\n",
    "            if lang == 'UD_Chinese-CFL':\n",
    "                continue\n",
    "            rel_treebanks.append(lang)\n",
    "            for attr in listdir(join(DEFAULT_RESULTS_FOLDER, lang)):\n",
    "                RESULTS.append((lang, attr))\n",
    "                \n",
    "        # embedding, attribute, language\n",
    "\n",
    "        DEFAULT_FILE_FORMAT = join(DEFAULT_RESULTS_FOLDER, \"{lang}/{attribute}/loginfo.json\")\n",
    "\n",
    "        results_raw: List[Tuple[Dict[str, Any], List[int]]] = []\n",
    "\n",
    "        for l, a in RESULTS:  # noqa\n",
    "            if l in rel_treebanks:\n",
    "                with open(DEFAULT_FILE_FORMAT.format(lang=l, attribute=a), \"r\") as h:\n",
    "                    data = json.load(h)\n",
    "\n",
    "                results_raw.append(\n",
    "                    (\n",
    "                        { \"embedding\": embedding, \"attribute\": a,\n",
    "                        \"language\": convert_language_code(l) },\n",
    "                        [d[\"iteration_dimension\"] for d in data if \"iteration_dimension\" in d]\n",
    "                        if not random_baseline else random.sample(range(embedding_size), k=top_k)\n",
    "                    )\n",
    "            )\n",
    "        # Compute p values\n",
    "        num_permutations = 1000000\n",
    "        p_vals_cache_file = f\"{embedding}_{top_k}_{num_permutations}_pvals.pkl\"\n",
    "        if not os.path.exists(p_vals_cache_file):\n",
    "            # Compute p-values for different similarities\n",
    "            # if embedding in [\"bert-base-multilingual-cased\", \"xlm-roberta-base\"]:\n",
    "            #     dimensionality = 768\n",
    "            # elif embedding == \"xlm-roberta-large\":\n",
    "            #     dimensionality = 1024\n",
    "            # elif 'bloom' in embedding:\n",
    "            #     dimensionality = 1024\n",
    "            # else:\n",
    "            #     raise Exception(\"Embedding does not exist\")\n",
    "            #     dimensionality = 300\n",
    "            dimensionality = embedding_size\n",
    "\n",
    "            reference_order = random.sample(list(range(dimensionality)), dimensionality)\n",
    "            reference_top_k = reference_order[:top_k]\n",
    "            similarities = []\n",
    "\n",
    "            for i in tqdm(range(num_permutations)):\n",
    "                permuted_top_k = random.sample(reference_order, top_k)\n",
    "                similarities.append(compute_overlap_coefficient(set(reference_top_k), set(permuted_top_k)))\n",
    "                pvals = {}\n",
    "\n",
    "            for i in range(top_k + 1):\n",
    "                observed_hypothesis = i / top_k  # What is overlap score greater than or equal to?\n",
    "                permutations_match = [s for s in similarities if s >= observed_hypothesis]\n",
    "                pval = len(permutations_match) / num_permutations\n",
    "                print(f\"P-value when sim >= {observed_hypothesis} (overlap >= {i} dims): {pval:.5f}\")\n",
    "                pvals[i] = pval\n",
    "\n",
    "            with open(p_vals_cache_file, \"wb\") as h:\n",
    "                pickle.dump(pvals, h)\n",
    "        else:\n",
    "            with open(p_vals_cache_file, \"rb\") as h:\n",
    "                pvals = pickle.load(h)\n",
    "\n",
    "        for attr in attributes:\n",
    "            labels, (similarity_matrix, extra_data) = compute_similarity_for_attribute(\n",
    "                attr, results_raw, top_k)\n",
    "            \n",
    "            # get the average pairwise overlap:\n",
    "            print(len(labels))\n",
    "            print(similarity_matrix)\n",
    "            avg_overlap_rates[attr][layer] = np.average(similarity_matrix[np.triu_indices(len(labels), k = 1)])\n",
    "            \n",
    "\n",
    "            x_labels = labels\n",
    "            y_labels = labels\n",
    "\n",
    "            p_values_matrix = compute_pvalues(extra_data[\"overlap_num\"], pvals)\n",
    "            annotation_matrix = build_statistical_significance_matrix(\n",
    "                p_values_matrix, alpha=0.05, method=\"holm-bonferroni\", symmetry=True)\n",
    "    \n",
    "    for attr, overlap_rates in avg_overlap_rates.items():\n",
    "        overlap_rates = sorted(overlap_rates.items()) \n",
    "        print(overlap_rates)\n",
    "        x, y = zip(*overlap_rates)\n",
    "        \n",
    "        plt.plot(x, y, f'o-', label=attr)\n",
    "\n",
    "\n",
    "elif checkpoints is not None:\n",
    "    print(\"Loop over checkpoints\")\n",
    "    for checkpoint in checkpoints:\n",
    "        embedding = f\"{model}-intermediate-global_step{checkpoint}\"\n",
    "        if checkpoint == 'best':\n",
    "            embedding = model\n",
    "        DEFAULT_RESULTS_FOLDER = f\"results/{embedding}/{experiment_name}\"\n",
    "\n",
    "        # print(listdir(DEFAULT_RESULTS_FOLDER))\n",
    "        RESULTS = []\n",
    "        rel_treebanks = []\n",
    "        for lang in listdir(DEFAULT_RESULTS_FOLDER):\n",
    "            if lang == 'UD_Chinese-CFL':\n",
    "                continue\n",
    "            rel_treebanks.append(lang)\n",
    "            for attr in listdir(join(DEFAULT_RESULTS_FOLDER, lang)):\n",
    "                RESULTS.append((lang, attr))\n",
    "                \n",
    "        # embedding, attribute, language\n",
    "\n",
    "        DEFAULT_FILE_FORMAT = join(DEFAULT_RESULTS_FOLDER, \"{lang}/{attribute}/loginfo.json\")\n",
    "\n",
    "        results_raw: List[Tuple[Dict[str, Any], List[int]]] = []\n",
    "\n",
    "        for l, a in RESULTS:  # noqa\n",
    "            if l in rel_treebanks:\n",
    "                with open(DEFAULT_FILE_FORMAT.format(lang=l, attribute=a), \"r\") as h:\n",
    "                    data = json.load(h)\n",
    "\n",
    "                results_raw.append(\n",
    "                    (\n",
    "                        { \"embedding\": embedding, \"attribute\": a,\n",
    "                        \"language\": convert_language_code(l) },\n",
    "                        [d[\"iteration_dimension\"] for d in data if \"iteration_dimension\" in d]\n",
    "                        if not random_baseline else random.sample(range(embedding_size), k=top_k)\n",
    "                    )\n",
    "            )\n",
    "        # Compute p values\n",
    "        num_permutations = 1000000\n",
    "        p_vals_cache_file = f\"{embedding}_{top_k}_{num_permutations}_pvals.pkl\"\n",
    "        if not os.path.exists(p_vals_cache_file):\n",
    "            # Compute p-values for different similarities\n",
    "            # if embedding in [\"bert-base-multilingual-cased\", \"xlm-roberta-base\"]:\n",
    "            #     dimensionality = 768\n",
    "            # elif embedding == \"xlm-roberta-large\":\n",
    "            #     dimensionality = 1024\n",
    "            # elif 'bloom' in embedding:\n",
    "            #     dimensionality = 1024\n",
    "            # else:\n",
    "            #     raise Exception(\"Embedding does not exist\")\n",
    "            #     dimensionality = 300\n",
    "            dimensionality = embedding_size\n",
    "\n",
    "            reference_order = random.sample(list(range(dimensionality)), dimensionality)\n",
    "            reference_top_k = reference_order[:top_k]\n",
    "            similarities = []\n",
    "\n",
    "            for i in tqdm(range(num_permutations)):\n",
    "                permuted_top_k = random.sample(reference_order, top_k)\n",
    "                similarities.append(compute_overlap_coefficient(set(reference_top_k), set(permuted_top_k)))\n",
    "                pvals = {}\n",
    "\n",
    "            for i in range(top_k + 1):\n",
    "                observed_hypothesis = i / top_k  # What is overlap score greater than or equal to?\n",
    "                permutations_match = [s for s in similarities if s >= observed_hypothesis]\n",
    "                pval = len(permutations_match) / num_permutations\n",
    "                print(f\"P-value when sim >= {observed_hypothesis} (overlap >= {i} dims): {pval:.5f}\")\n",
    "                pvals[i] = pval\n",
    "\n",
    "            with open(p_vals_cache_file, \"wb\") as h:\n",
    "                pickle.dump(pvals, h)\n",
    "        else:\n",
    "            with open(p_vals_cache_file, \"rb\") as h:\n",
    "                pvals = pickle.load(h)\n",
    "\n",
    "        for attr in attributes:\n",
    "            labels, (similarity_matrix, extra_data) = compute_similarity_for_attribute(\n",
    "                attr, results_raw, top_k)\n",
    "            \n",
    "            # get the average pairwise overlap:\n",
    "            print(labels)\n",
    "            print(similarity_matrix)\n",
    "            if attr == 'POS':\n",
    "                ovlp_rate_with_eng = similarity_matrix[labels.index('eng')].tolist()\n",
    "                print(ovlp_rate_with_eng)\n",
    "                print(type(ovlp_rate_with_eng))\n",
    "                for lang, ovlp_rate in zip(labels, ovlp_rate_with_eng):\n",
    "                    pos_ovlp_dict[checkpoint][lang] = ovlp_rate\n",
    "\n",
    "            if checkpoint == 'best':\n",
    "                avg_overlap_rates[attr][checkpoint] = np.average(similarity_matrix[np.triu_indices(len(labels), k = 1)])\n",
    "            else:\n",
    "                avg_overlap_rates[attr][checkpoint] = np.average(similarity_matrix[np.triu_indices(len(labels), k = 1)])\n",
    "\n",
    "            x_labels = labels\n",
    "            y_labels = labels\n",
    "\n",
    "            p_values_matrix = compute_pvalues(extra_data[\"overlap_num\"], pvals)\n",
    "            annotation_matrix = build_statistical_significance_matrix(\n",
    "                p_values_matrix, alpha=0.05, method=\"holm-bonferroni\", symmetry=True)\n",
    "    \n",
    "    for attr, overlap_rates in avg_overlap_rates.items():\n",
    "        \n",
    "        if 'best' in overlap_rates.keys():\n",
    "            rate_for_best_checkpoint = [overlap_rates['best']]*8\n",
    "            # del overlap_rates['best']\n",
    "            # plt.plot(x, rate_for_best_checkpoint, f'-{color}', label=f'{model}-{attr}')\n",
    "            \n",
    "        # overlap_rates = sorted(overlap_rates.items()) \n",
    "        # x, y = zip(*overlap_rates)\n",
    "        \n",
    "        shape = shapes[attr]\n",
    "        \n",
    "        # ax2.plot(x, y, f'{shape}--', label=attr)      \n",
    "    \n",
    "else: \n",
    "    print(\"no x axises. please check.\")\n",
    "\n",
    "print(avg_overlap_rates)\n",
    "print(pos_ovlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 0.1342079772079772, '1000': 0.04538909238909239, '10000': 0.04760805860805861, '50000': 0.06011884411884413, '100000': 0.0765050875050875, '150000': 0.08755718355718356, '200000': 0.09470288970288969, '250000': 0.10913268213268212, '300000': 0.11238298738298737}\n"
     ]
    }
   ],
   "source": [
    "ckpts_avg_overlap_rates = {}\n",
    "\n",
    "for ckpt in checkpoints:\n",
    "    ckpt_sum_rates = 0\n",
    "    for attr in attributes:\n",
    "        ckpt_sum_rates += avg_overlap_rates[attr][ckpt]\n",
    "    ckpts_avg_overlap_rates[ckpt] = ckpt_sum_rates/len(attributes)\n",
    "\n",
    "print(ckpts_avg_overlap_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"f1_rate_pairwise.txt\", 'a+') as f: \n",
    "    for ckpt in checkpoints:\n",
    "        for lang, f1_score in ckpt_lan_f1_dict[ckpt].items():\n",
    "            f.write('%s %s %s\\n' % (f1_score, pos_ovlp_dict[ckpt][lang_code_dict[lang]], ckpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"f1_rate_ckpts.txt\", 'a+') as f: \n",
    "#     for ckpt in checkpoints:\n",
    "#         f.write('%s %s %s\\n' % (avg_f1_dict[ckpt], ckpts_avg_overlap_rates[ckpt], ckpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.legend(loc='center left', bbox_to_anchor=(1.2, 0.5))\n",
    "# ax2.set_ylabel('overlap rates')\n",
    "# fig.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "# # fig.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "# if layers is not None:\n",
    "#     fig.xlabel(\"layer\")\n",
    "    \n",
    "# plt.style.use('seaborn-darkgrid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.tight_layout()\n",
    "# sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if layers is not None:\n",
    "#     plt.savefig(f'experiments/scatterplots/{model}/layers.pdf')\n",
    "# elif checkpoints is not None:\n",
    "#     plt.savefig(f'experiments/scatterplots/{model}/checkpoints_{experiment_name}.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-typology-probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed44db9dbf3c575ec3ddf9da9744b6e2502eab3ed89ccbe6b1ade08a3a1a68bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
