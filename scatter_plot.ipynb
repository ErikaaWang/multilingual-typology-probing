{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============imports===================\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Set, Optional, Any\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "\n",
    "import pycountry\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "from collections import defaultdict\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================args=====================\n",
    "# checkpoints = 'best'\n",
    "checkpoints = None\n",
    "# checkpoints = ['50000', '100000', '150000', '200000']\n",
    "checkpoints = ['1000', '100000', '200000', '300000', '400000', '600000']\n",
    "# checkpoints = ['1000', '10000', '100000', '200000', '300000', '400000', '500000', '600000']\n",
    "random_baseline = False\n",
    "# embedding_size = 1024\n",
    "top_k = 50\n",
    "attributes = ['Gender', 'Number', 'POS']\n",
    "shapes = {'Gender':'^', 'Number':'o', 'POS':'s'}\n",
    "# attributes = ['Aspect', 'Case', 'Definiteness', 'Finiteness', 'Gender', 'Mood', 'Number',\\\n",
    "#              'Person', 'POS', 'Tense', 'Voice']\n",
    "language = None\n",
    "show_plot = False\n",
    "experiment_name = 'inter-layer-17'\n",
    "# layers = [1, 5, 9, 13, 17, 21, 25]\n",
    "layers = None\n",
    "model = 'bloom-560m'\n",
    "sns.set_theme()\n",
    "\n",
    "# cross-lingual eval result args\n",
    "output_file_path = 'xtreme/outputs-temp/udpos/'\n",
    "output_file_suffix = '-LR2e-5-epoch10-MaxLen128'\n",
    "langs = ['ar', 'eu', 'ca', 'zh', 'en', 'fr', 'hi', 'mr', 'pt', 'es', 'ta', 'ur', 'vi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================functions====================\n",
    "def convert_language_code(treebank_name):\n",
    "    \"\"\" Converts treebank names to language codes. \"\"\"\n",
    "    lang_name = treebank_name[3:].split(\"-\")[0].replace(\"_\", \" \")\n",
    "    lang = pycountry.languages.get(name=lang_name)\n",
    "\n",
    "    if lang is not None:\n",
    "        return lang.alpha_3.lower()\n",
    "\n",
    "    return \"unk\"\n",
    "\n",
    "def compute_overlap(data_raw, top_k):\n",
    "    mark_count: Dict[str, int] = {}  # num of languages logging that attribute\n",
    "    results: Dict[str, Counter] = {}\n",
    "    for run_config, dims in data_raw:\n",
    "        if run_config[\"embedding\"] != embedding:\n",
    "            continue\n",
    "\n",
    "        # Increment mark counting\n",
    "        if run_config[\"attribute\"] not in mark_count:\n",
    "            mark_count[run_config[\"attribute\"]] = 0\n",
    "\n",
    "        mark_count[run_config[\"attribute\"]] += 1\n",
    "\n",
    "        # Increment actual counters\n",
    "        if run_config[\"attribute\"] not in results:\n",
    "            results[run_config[\"attribute\"]] = Counter()\n",
    "\n",
    "        results[run_config[\"attribute\"]].update(dims[:top_k])\n",
    "\n",
    "    return results, mark_count\n",
    "\n",
    "\n",
    "def compute_similarity_for_attribute(attribute, data_raw, top_k, language_order: Optional[List[str]] = None):\n",
    "    data_list: List[Tuple[str, Set[int]]] = []\n",
    "    for run_config, dims in data_raw:\n",
    "        if run_config[\"embedding\"] != embedding:\n",
    "            continue\n",
    "\n",
    "        if run_config[\"attribute\"] != attribute:\n",
    "            continue\n",
    "\n",
    "        data_list.append((run_config[\"language\"], set(dims[:top_k])))\n",
    "\n",
    "    if not language_order:\n",
    "        data_list = sorted(data_list, key=lambda x: x[0])\n",
    "        return [x[0] for x in data_list], compute_similarity(data_list)\n",
    "    else:\n",
    "        data_list_dict = {k: v for k, v in data_list}\n",
    "        data_list_sorted = []\n",
    "        for x in language_order:\n",
    "            if x in data_list_dict:\n",
    "                data_list_sorted.append((x, data_list_dict[x]))\n",
    "\n",
    "        data_list = data_list_sorted\n",
    "        return [x[0] for x in data_list], compute_similarity(data_list)\n",
    "\n",
    "\n",
    "def compute_similarity_for_language(language, data_raw, top_k):\n",
    "    data_list: List[Tuple[str, Set[int]]] = []\n",
    "    for run_config, dims in data_raw:\n",
    "        if run_config[\"embedding\"] != embedding:\n",
    "            continue\n",
    "\n",
    "        if run_config[\"language\"] != language:\n",
    "            continue\n",
    "\n",
    "        data_list.append((run_config[\"attribute\"], set(dims[:top_k])))\n",
    "\n",
    "    data_list = sorted(data_list, key=lambda x: x[0])\n",
    "\n",
    "    return [x[0] for x in data_list], compute_similarity(data_list)\n",
    "\n",
    "\n",
    "def compute_jaccard_index(set_a: Set[int], set_b: Set[int]) -> float:\n",
    "    return len(set_a & set_b) / len(set_a | set_b)\n",
    "\n",
    "\n",
    "def compute_overlap_coefficient(set_a: Set[int], set_b: Set[int]) -> float:\n",
    "    return len(set_a & set_b) / min(len(set_a), len(set_b))\n",
    "\n",
    "\n",
    "def compute_similarity(data_list: List[Tuple[str, Set[int]]]):\n",
    "    num_items = len(data_list)\n",
    "    similarity_array = np.zeros((num_items, num_items))\n",
    "    extra_data = {\n",
    "        \"overlap\": np.empty((num_items, num_items), dtype=list),\n",
    "        \"overlap_num\": np.zeros((num_items, num_items)),\n",
    "    }\n",
    "    for idx_a, (group_a, dim_set_a) in enumerate(data_list):\n",
    "        for idx_b, (group_b, dim_set_b) in enumerate(data_list):\n",
    "            similarity_array[idx_a, idx_b] = compute_overlap_coefficient(dim_set_a, dim_set_b)\n",
    "            extra_data[\"overlap\"][idx_a, idx_b] = sorted(list(set(dim_set_a & dim_set_b)))\n",
    "            extra_data[\"overlap_num\"][idx_a, idx_b] = len(dim_set_a & dim_set_b)\n",
    "\n",
    "    return similarity_array, extra_data\n",
    "\n",
    "\n",
    "def compute_pvalues(overlap_num_matrix: np.array, p_val_dict: Dict[int, float]) -> np.array:\n",
    "    return np.vectorize(lambda x: p_val_dict[int(x)])(overlap_num_matrix)\n",
    "\n",
    "\n",
    "def build_statistical_significance_matrix(p_values_matrix, alpha=0.05, method=\"bonferroni\", symmetry=False):\n",
    "    num_rows = p_values_matrix.shape[0]\n",
    "    num_hypotheses = int(num_rows * (num_rows + 1) / 2) - num_rows\n",
    "    num_hypotheses = num_hypotheses if num_hypotheses > 0 else 999\n",
    "    alpha_bonferroni = alpha / num_hypotheses\n",
    "\n",
    "    if method == \"bonferroni\":\n",
    "        mask = np.tril(np.ones_like(p_values_matrix, dtype=bool), k=-1)\n",
    "        significance_matrix = (p_values_matrix < alpha_bonferroni) * mask\n",
    "    elif method == \"holm-bonferroni\":\n",
    "        mask = np.triu(np.ones_like(p_values_matrix)) * 9999.0\n",
    "        p_values_matrix += mask\n",
    "        p_values_matrix_flat = p_values_matrix.reshape(-1)\n",
    "        sorting_indices = p_values_matrix_flat.argsort()\n",
    "        unsorting_indices = sorting_indices.argsort()\n",
    "\n",
    "        sorted_p_values = p_values_matrix_flat[sorting_indices][:num_hypotheses]\n",
    "        alpha_holm = np.arange(1.0, num_hypotheses + 1.0)[::-1] ** -1 * alpha\n",
    "\n",
    "        broke = False\n",
    "        for k, (pval, alph) in enumerate(zip(sorted_p_values.tolist(), alpha_holm.tolist())):\n",
    "            if pval > alph:\n",
    "                broke = True\n",
    "                break\n",
    "\n",
    "        if not broke:\n",
    "            # Needed in case we never accepted the null hypothesis\n",
    "            k += 1\n",
    "\n",
    "        # k will be equal to the first index where we do NOT reject the null hypothesis.\n",
    "        # So we can accept the alternative hypothesis on all indices less than k\n",
    "        # e.g., if k == 0, we always accept the null hypothesis. If k == num_hypothesis\n",
    "        # we always reject the null hypothesis.\n",
    "        rejected_null_sorted = [True if idx < k else False for idx in range(num_hypotheses)]\n",
    "\n",
    "        # Pad remaining list with rejections\n",
    "        rejected_null_sorted.extend([False] * (num_rows ** 2 - num_hypotheses))\n",
    "\n",
    "        # Reverse sort\n",
    "        significance_matrix = np.array(rejected_null_sorted)[unsorting_indices].reshape(num_rows, num_rows)\n",
    "\n",
    "    if symmetry:\n",
    "        # Mirror along diagonal\n",
    "        significance_matrix = significance_matrix | significance_matrix.T\n",
    "\n",
    "    return significance_matrix\n",
    "\n",
    "\n",
    "def build_annotations_list(annotation_matrix):\n",
    "    n = annotation_matrix.shape[0]\n",
    "    annotation_list = []\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            if not annotation_matrix[x][y]:\n",
    "                continue\n",
    "\n",
    "            if x == y:\n",
    "                continue\n",
    "\n",
    "            annotation_list.append(\n",
    "                dict(\n",
    "                    x=x / n, y=y / n,\n",
    "                    xref='paper',\n",
    "                    yref='paper',\n",
    "                    text=\"■\",\n",
    "                    showarrow=False,\n",
    "                    xanchor=\"left\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    font=dict(color=\"rgb(236,136,106)\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return annotation_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f43c86eae20>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================read f1 scores==============================\n",
    "f1_dict = pd.read_csv(f'csv_files/{model}_f1-score.csv', index_col=0).to_dict()\n",
    "\n",
    "avg_f1_dict = {}\n",
    "fig, ax1 = plt.subplots(figsize=(5.5, 5))\n",
    "plt.ioff()\n",
    "\n",
    "for ckpt, f1_scores in f1_dict.items():\n",
    "    if ckpt == 'best':\n",
    "        continue\n",
    "    avg_f1_dict[int(ckpt)] = sum(f1_scores.values())/len(f1_scores.values())*100\n",
    "\n",
    "# for ckpt in checkpoints:\n",
    "#     f1_dict = {}\n",
    "#     ckpt_affix = '-intermediate-global_step' + ckpt\n",
    "#     output_file_name = os.path.join(output_file_path, model + ckpt_affix + output_file_suffix, 'test_results.txt')\n",
    "#     with open(output_file_name, 'r') as f:\n",
    "#         while True:\n",
    "#             line = f.readline()\n",
    "#             if not line:\n",
    "#                 break\n",
    "#             if 'language' in line:\n",
    "#                 lang = line.split('=')[1].split('\\n')[0]\n",
    "#                 f1_score = float(f.readline().split(' = ')[1])\n",
    "#                 if lang in langs:\n",
    "#                     f1_dict[lang] = f1_score \n",
    "#         print(f1_dict)\n",
    "#         avg_f1_score = sum(f1_dict.values())/len(f1_dict.values()) * 100\n",
    "#         avg_f1_dict[int(ckpt)] = avg_f1_score\n",
    "\n",
    "# plotting\n",
    "ckpts, f1_scores = zip(*avg_f1_dict.items())\n",
    "ax1.set_xlabel('global steps')\n",
    "ax1.set_ylabel('F1 scores', color='r')\n",
    "# ax1.set_ylim(30, 50)\n",
    "ax1.tick_params(axis='y', labelcolor='r', grid_alpha=0.5)\n",
    "ax1.plot(ckpts, f1_scores, 'r-')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ======================read cos similarities==============================\n",
    "# fig, ax1 = plt.subplots(figsize=(5.5, 5))\n",
    "# plt.ioff()\n",
    "# for layer in ['last-layer', experiment_name]:\n",
    "\n",
    "#     cos_dict = pd.read_csv(f'csv_files/{model}_{layer}_cos-similarity.csv', index_col=0).to_dict()\n",
    "\n",
    "#     avg_cos_dict = {}\n",
    "#     code_cos_dict = {}    \n",
    "\n",
    "#     for ckpt, cos_similarity in cos_dict.items():\n",
    "#         if ckpt == 'best':\n",
    "#             continue\n",
    "#         code_cos_dict[int(ckpt)] = cos_similarity['code']\n",
    "#         del cos_similarity['code']\n",
    "#         avg_cos_dict[int(ckpt)] = sum(cos_similarity.values())/len(cos_similarity.values())\n",
    "\n",
    "#     # plotting\n",
    "#     ckpts, cos_similarities = zip(*avg_cos_dict.items())\n",
    "#     code_cos_similarities = code_cos_dict.values()\n",
    "#     ax1.set_xlabel('global steps')\n",
    "#     ax1.set_ylabel('Cosine Similarities')\n",
    "#     ax1.tick_params(axis='y', grid_alpha=0.5)\n",
    "#     if layer == 'last-layer':         \n",
    "#         ax1.plot(ckpts, cos_similarities, 'm-', label='nl-last-layer')\n",
    "#         ax1.plot(ckpts, code_cos_similarities, '-', color='olive', label='code-last-layer')\n",
    "#     else:\n",
    "#         ax1.plot(ckpts, cos_similarities, '-', color='brown', label='nl')\n",
    "#         ax1.plot(ckpts, code_cos_similarities, 'c-', label='code')        \n",
    "#     plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop over checkpoints\n",
      "8\n",
      "[[1.   0.08 0.02 0.1  0.02 0.04 0.08 0.08]\n",
      " [0.08 1.   0.2  0.06 0.04 0.1  0.14 0.04]\n",
      " [0.02 0.2  1.   0.02 0.04 0.14 0.16 0.06]\n",
      " [0.1  0.06 0.02 1.   0.04 0.04 0.04 0.  ]\n",
      " [0.02 0.04 0.04 0.04 1.   0.12 0.04 0.12]\n",
      " [0.04 0.1  0.14 0.04 0.12 1.   0.34 0.06]\n",
      " [0.08 0.14 0.16 0.04 0.04 0.34 1.   0.06]\n",
      " [0.08 0.04 0.06 0.   0.12 0.06 0.06 1.  ]]\n",
      "11\n",
      "[[1.   0.06 0.   0.04 0.04 0.06 0.04 0.06 0.02 0.1  0.02]\n",
      " [0.06 1.   0.18 0.06 0.32 0.   0.04 0.3  0.3  0.06 0.02]\n",
      " [0.   0.18 1.   0.06 0.16 0.02 0.04 0.04 0.1  0.08 0.04]\n",
      " [0.04 0.06 0.06 1.   0.08 0.   0.02 0.04 0.04 0.   0.1 ]\n",
      " [0.04 0.32 0.16 0.08 1.   0.   0.06 0.12 0.24 0.08 0.02]\n",
      " [0.06 0.   0.02 0.   0.   1.   0.06 0.06 0.06 0.06 0.08]\n",
      " [0.04 0.04 0.04 0.02 0.06 0.06 1.   0.02 0.06 0.06 0.12]\n",
      " [0.06 0.3  0.04 0.04 0.12 0.06 0.02 1.   0.28 0.06 0.06]\n",
      " [0.02 0.3  0.1  0.04 0.24 0.06 0.06 0.28 1.   0.02 0.  ]\n",
      " [0.1  0.06 0.08 0.   0.08 0.06 0.06 0.06 0.02 1.   0.02]\n",
      " [0.02 0.02 0.04 0.1  0.02 0.08 0.12 0.06 0.   0.02 1.  ]]\n",
      "13\n",
      "[[1.   0.08 0.06 0.06 0.04 0.04 0.04 0.02 0.02 0.02 0.1  0.08 0.02]\n",
      " [0.08 1.   0.04 0.14 0.2  0.   0.06 0.02 0.04 0.06 0.04 0.08 0.1 ]\n",
      " [0.06 0.04 1.   0.06 0.16 0.06 0.02 0.1  0.06 0.06 0.06 0.08 0.06]\n",
      " [0.06 0.14 0.06 1.   0.06 0.1  0.02 0.08 0.08 0.04 0.06 0.08 0.12]\n",
      " [0.04 0.2  0.16 0.06 1.   0.   0.   0.04 0.1  0.1  0.04 0.06 0.1 ]\n",
      " [0.04 0.   0.06 0.1  0.   1.   0.02 0.04 0.04 0.06 0.02 0.06 0.04]\n",
      " [0.04 0.06 0.02 0.02 0.   0.02 1.   0.   0.04 0.08 0.04 0.06 0.04]\n",
      " [0.02 0.02 0.1  0.08 0.04 0.04 0.   1.   0.08 0.06 0.06 0.06 0.12]\n",
      " [0.02 0.04 0.06 0.08 0.1  0.04 0.04 0.08 1.   0.12 0.1  0.08 0.1 ]\n",
      " [0.02 0.06 0.06 0.04 0.1  0.06 0.08 0.06 0.12 1.   0.08 0.04 0.06]\n",
      " [0.1  0.04 0.06 0.06 0.04 0.02 0.04 0.06 0.1  0.08 1.   0.06 0.06]\n",
      " [0.08 0.08 0.08 0.08 0.06 0.06 0.06 0.06 0.08 0.04 0.06 1.   0.1 ]\n",
      " [0.02 0.1  0.06 0.12 0.1  0.04 0.04 0.12 0.1  0.06 0.06 0.1  1.  ]]\n",
      "8\n",
      "[[1.   0.24 0.32 0.14 0.02 0.26 0.22 0.08]\n",
      " [0.24 1.   0.36 0.06 0.02 0.26 0.3  0.04]\n",
      " [0.32 0.36 1.   0.1  0.08 0.3  0.3  0.04]\n",
      " [0.14 0.06 0.1  1.   0.08 0.14 0.1  0.12]\n",
      " [0.02 0.02 0.08 0.08 1.   0.04 0.08 0.04]\n",
      " [0.26 0.26 0.3  0.14 0.04 1.   0.46 0.08]\n",
      " [0.22 0.3  0.3  0.1  0.08 0.46 1.   0.04]\n",
      " [0.08 0.04 0.04 0.12 0.04 0.08 0.04 1.  ]]\n",
      "11\n",
      "[[1.   0.08 0.1  0.16 0.1  0.08 0.04 0.14 0.06 0.1  0.08]\n",
      " [0.08 1.   0.16 0.1  0.36 0.18 0.08 0.3  0.36 0.1  0.14]\n",
      " [0.1  0.16 1.   0.12 0.26 0.16 0.08 0.14 0.2  0.1  0.16]\n",
      " [0.16 0.1  0.12 1.   0.1  0.12 0.08 0.14 0.06 0.12 0.22]\n",
      " [0.1  0.36 0.26 0.1  1.   0.18 0.02 0.32 0.34 0.08 0.18]\n",
      " [0.08 0.18 0.16 0.12 0.18 1.   0.08 0.26 0.18 0.12 0.18]\n",
      " [0.04 0.08 0.08 0.08 0.02 0.08 1.   0.04 0.06 0.04 0.08]\n",
      " [0.14 0.3  0.14 0.14 0.32 0.26 0.04 1.   0.4  0.1  0.14]\n",
      " [0.06 0.36 0.2  0.06 0.34 0.18 0.06 0.4  1.   0.1  0.06]\n",
      " [0.1  0.1  0.1  0.12 0.08 0.12 0.04 0.1  0.1  1.   0.18]\n",
      " [0.08 0.14 0.16 0.22 0.18 0.18 0.08 0.14 0.06 0.18 1.  ]]\n",
      "13\n",
      "[[1.   0.1  0.08 0.02 0.1  0.06 0.04 0.12 0.12 0.06 0.12 0.08 0.14]\n",
      " [0.1  1.   0.1  0.08 0.14 0.04 0.02 0.18 0.18 0.04 0.08 0.06 0.08]\n",
      " [0.08 0.1  1.   0.06 0.18 0.06 0.02 0.14 0.2  0.06 0.06 0.1  0.1 ]\n",
      " [0.02 0.08 0.06 1.   0.18 0.08 0.08 0.1  0.12 0.06 0.08 0.04 0.1 ]\n",
      " [0.1  0.14 0.18 0.18 1.   0.12 0.02 0.26 0.22 0.12 0.06 0.14 0.18]\n",
      " [0.06 0.04 0.06 0.08 0.12 1.   0.06 0.08 0.06 0.04 0.08 0.06 0.12]\n",
      " [0.04 0.02 0.02 0.08 0.02 0.06 1.   0.04 0.02 0.08 0.08 0.04 0.  ]\n",
      " [0.12 0.18 0.14 0.1  0.26 0.08 0.04 1.   0.28 0.06 0.02 0.16 0.14]\n",
      " [0.12 0.18 0.2  0.12 0.22 0.06 0.02 0.28 1.   0.04 0.02 0.08 0.18]\n",
      " [0.06 0.04 0.06 0.06 0.12 0.04 0.08 0.06 0.04 1.   0.1  0.06 0.08]\n",
      " [0.12 0.08 0.06 0.08 0.06 0.08 0.08 0.02 0.02 0.1  1.   0.04 0.14]\n",
      " [0.08 0.06 0.1  0.04 0.14 0.06 0.04 0.16 0.08 0.06 0.04 1.   0.06]\n",
      " [0.14 0.08 0.1  0.1  0.18 0.12 0.   0.14 0.18 0.08 0.14 0.06 1.  ]]\n",
      "8\n",
      "[[1.   0.2  0.28 0.14 0.08 0.26 0.24 0.04]\n",
      " [0.2  1.   0.32 0.12 0.06 0.3  0.3  0.1 ]\n",
      " [0.28 0.32 1.   0.22 0.08 0.36 0.42 0.08]\n",
      " [0.14 0.12 0.22 1.   0.08 0.24 0.22 0.12]\n",
      " [0.08 0.06 0.08 0.08 1.   0.04 0.08 0.06]\n",
      " [0.26 0.3  0.36 0.24 0.04 1.   0.44 0.  ]\n",
      " [0.24 0.3  0.42 0.22 0.08 0.44 1.   0.06]\n",
      " [0.04 0.1  0.08 0.12 0.06 0.   0.06 1.  ]]\n",
      "11\n",
      "[[1.   0.12 0.18 0.08 0.12 0.16 0.02 0.1  0.12 0.1  0.14]\n",
      " [0.12 1.   0.16 0.2  0.34 0.18 0.   0.24 0.3  0.14 0.22]\n",
      " [0.18 0.16 1.   0.08 0.18 0.12 0.04 0.12 0.16 0.1  0.12]\n",
      " [0.08 0.2  0.08 1.   0.1  0.16 0.06 0.14 0.14 0.16 0.14]\n",
      " [0.12 0.34 0.18 0.1  1.   0.2  0.02 0.34 0.32 0.2  0.14]\n",
      " [0.16 0.18 0.12 0.16 0.2  1.   0.08 0.28 0.16 0.12 0.24]\n",
      " [0.02 0.   0.04 0.06 0.02 0.08 1.   0.02 0.02 0.1  0.04]\n",
      " [0.1  0.24 0.12 0.14 0.34 0.28 0.02 1.   0.34 0.1  0.16]\n",
      " [0.12 0.3  0.16 0.14 0.32 0.16 0.02 0.34 1.   0.14 0.12]\n",
      " [0.1  0.14 0.1  0.16 0.2  0.12 0.1  0.1  0.14 1.   0.1 ]\n",
      " [0.14 0.22 0.12 0.14 0.14 0.24 0.04 0.16 0.12 0.1  1.  ]]\n",
      "13\n",
      "[[1.   0.06 0.1  0.1  0.14 0.12 0.08 0.12 0.2  0.04 0.08 0.08 0.12]\n",
      " [0.06 1.   0.08 0.08 0.22 0.06 0.04 0.18 0.08 0.04 0.04 0.04 0.08]\n",
      " [0.1  0.08 1.   0.1  0.32 0.04 0.04 0.18 0.12 0.   0.06 0.22 0.18]\n",
      " [0.1  0.08 0.1  1.   0.06 0.   0.04 0.04 0.04 0.02 0.06 0.02 0.06]\n",
      " [0.14 0.22 0.32 0.06 1.   0.04 0.04 0.36 0.18 0.   0.1  0.12 0.2 ]\n",
      " [0.12 0.06 0.04 0.   0.04 1.   0.04 0.08 0.1  0.16 0.24 0.   0.06]\n",
      " [0.08 0.04 0.04 0.04 0.04 0.04 1.   0.04 0.06 0.02 0.08 0.06 0.08]\n",
      " [0.12 0.18 0.18 0.04 0.36 0.08 0.04 1.   0.16 0.   0.08 0.14 0.2 ]\n",
      " [0.2  0.08 0.12 0.04 0.18 0.1  0.06 0.16 1.   0.02 0.08 0.06 0.2 ]\n",
      " [0.04 0.04 0.   0.02 0.   0.16 0.02 0.   0.02 1.   0.1  0.04 0.02]\n",
      " [0.08 0.04 0.06 0.06 0.1  0.24 0.08 0.08 0.08 0.1  1.   0.06 0.18]\n",
      " [0.08 0.04 0.22 0.02 0.12 0.   0.06 0.14 0.06 0.04 0.06 1.   0.14]\n",
      " [0.12 0.08 0.18 0.06 0.2  0.06 0.08 0.2  0.2  0.02 0.18 0.14 1.  ]]\n",
      "8\n",
      "[[1.   0.2  0.24 0.18 0.12 0.22 0.2  0.1 ]\n",
      " [0.2  1.   0.4  0.2  0.12 0.28 0.4  0.06]\n",
      " [0.24 0.4  1.   0.38 0.08 0.38 0.48 0.04]\n",
      " [0.18 0.2  0.38 1.   0.16 0.3  0.28 0.06]\n",
      " [0.12 0.12 0.08 0.16 1.   0.14 0.12 0.1 ]\n",
      " [0.22 0.28 0.38 0.3  0.14 1.   0.46 0.1 ]\n",
      " [0.2  0.4  0.48 0.28 0.12 0.46 1.   0.04]\n",
      " [0.1  0.06 0.04 0.06 0.1  0.1  0.04 1.  ]]\n",
      "11\n",
      "[[1.   0.12 0.2  0.06 0.14 0.12 0.04 0.14 0.12 0.12 0.1 ]\n",
      " [0.12 1.   0.2  0.04 0.34 0.26 0.1  0.34 0.42 0.08 0.18]\n",
      " [0.2  0.2  1.   0.08 0.26 0.16 0.06 0.2  0.16 0.04 0.18]\n",
      " [0.06 0.04 0.08 1.   0.12 0.08 0.08 0.12 0.08 0.18 0.1 ]\n",
      " [0.14 0.34 0.26 0.12 1.   0.28 0.08 0.26 0.36 0.18 0.2 ]\n",
      " [0.12 0.26 0.16 0.08 0.28 1.   0.08 0.28 0.3  0.06 0.18]\n",
      " [0.04 0.1  0.06 0.08 0.08 0.08 1.   0.1  0.06 0.02 0.06]\n",
      " [0.14 0.34 0.2  0.12 0.26 0.28 0.1  1.   0.46 0.08 0.16]\n",
      " [0.12 0.42 0.16 0.08 0.36 0.3  0.06 0.46 1.   0.08 0.14]\n",
      " [0.12 0.08 0.04 0.18 0.18 0.06 0.02 0.08 0.08 1.   0.08]\n",
      " [0.1  0.18 0.18 0.1  0.2  0.18 0.06 0.16 0.14 0.08 1.  ]]\n",
      "13\n",
      "[[1.   0.1  0.18 0.06 0.12 0.14 0.08 0.18 0.16 0.1  0.08 0.14 0.14]\n",
      " [0.1  1.   0.16 0.04 0.22 0.06 0.04 0.28 0.16 0.02 0.02 0.04 0.14]\n",
      " [0.18 0.16 1.   0.06 0.24 0.1  0.06 0.2  0.12 0.04 0.06 0.1  0.14]\n",
      " [0.06 0.04 0.06 1.   0.06 0.02 0.02 0.06 0.04 0.04 0.06 0.08 0.08]\n",
      " [0.12 0.22 0.24 0.06 1.   0.1  0.06 0.26 0.2  0.06 0.04 0.12 0.14]\n",
      " [0.14 0.06 0.1  0.02 0.1  1.   0.12 0.2  0.12 0.12 0.16 0.04 0.16]\n",
      " [0.08 0.04 0.06 0.02 0.06 0.12 1.   0.08 0.08 0.08 0.1  0.14 0.04]\n",
      " [0.18 0.28 0.2  0.06 0.26 0.2  0.08 1.   0.22 0.1  0.1  0.1  0.12]\n",
      " [0.16 0.16 0.12 0.04 0.2  0.12 0.08 0.22 1.   0.1  0.08 0.14 0.1 ]\n",
      " [0.1  0.02 0.04 0.04 0.06 0.12 0.08 0.1  0.1  1.   0.08 0.1  0.06]\n",
      " [0.08 0.02 0.06 0.06 0.04 0.16 0.1  0.1  0.08 0.08 1.   0.04 0.1 ]\n",
      " [0.14 0.04 0.1  0.08 0.12 0.04 0.14 0.1  0.14 0.1  0.04 1.   0.16]\n",
      " [0.14 0.14 0.14 0.08 0.14 0.16 0.04 0.12 0.1  0.06 0.1  0.16 1.  ]]\n",
      "8\n",
      "[[1.   0.26 0.26 0.22 0.14 0.24 0.26 0.02]\n",
      " [0.26 1.   0.36 0.24 0.08 0.32 0.3  0.06]\n",
      " [0.26 0.36 1.   0.26 0.08 0.36 0.42 0.06]\n",
      " [0.22 0.24 0.26 1.   0.1  0.34 0.28 0.12]\n",
      " [0.14 0.08 0.08 0.1  1.   0.12 0.1  0.04]\n",
      " [0.24 0.32 0.36 0.34 0.12 1.   0.44 0.04]\n",
      " [0.26 0.3  0.42 0.28 0.1  0.44 1.   0.08]\n",
      " [0.02 0.06 0.06 0.12 0.04 0.04 0.08 1.  ]]\n",
      "11\n",
      "[[1.   0.18 0.18 0.12 0.18 0.14 0.02 0.14 0.16 0.12 0.14]\n",
      " [0.18 1.   0.3  0.2  0.34 0.22 0.06 0.38 0.4  0.22 0.2 ]\n",
      " [0.18 0.3  1.   0.2  0.24 0.22 0.06 0.28 0.24 0.08 0.08]\n",
      " [0.12 0.2  0.2  1.   0.18 0.12 0.04 0.12 0.12 0.14 0.12]\n",
      " [0.18 0.34 0.24 0.18 1.   0.2  0.04 0.28 0.3  0.18 0.14]\n",
      " [0.14 0.22 0.22 0.12 0.2  1.   0.12 0.12 0.2  0.18 0.24]\n",
      " [0.02 0.06 0.06 0.04 0.04 0.12 1.   0.04 0.06 0.06 0.04]\n",
      " [0.14 0.38 0.28 0.12 0.28 0.12 0.04 1.   0.42 0.08 0.1 ]\n",
      " [0.16 0.4  0.24 0.12 0.3  0.2  0.06 0.42 1.   0.1  0.14]\n",
      " [0.12 0.22 0.08 0.14 0.18 0.18 0.06 0.08 0.1  1.   0.08]\n",
      " [0.14 0.2  0.08 0.12 0.14 0.24 0.04 0.1  0.14 0.08 1.  ]]\n",
      "13\n",
      "[[1.   0.2  0.16 0.06 0.18 0.22 0.04 0.22 0.2  0.1  0.16 0.18 0.14]\n",
      " [0.2  1.   0.2  0.04 0.36 0.06 0.04 0.28 0.26 0.04 0.06 0.16 0.14]\n",
      " [0.16 0.2  1.   0.1  0.2  0.12 0.08 0.22 0.16 0.06 0.1  0.24 0.28]\n",
      " [0.06 0.04 0.1  1.   0.04 0.04 0.06 0.06 0.08 0.12 0.06 0.14 0.1 ]\n",
      " [0.18 0.36 0.2  0.04 1.   0.1  0.06 0.36 0.32 0.06 0.14 0.18 0.18]\n",
      " [0.22 0.06 0.12 0.04 0.1  1.   0.08 0.2  0.1  0.16 0.18 0.1  0.08]\n",
      " [0.04 0.04 0.08 0.06 0.06 0.08 1.   0.02 0.08 0.1  0.04 0.04 0.04]\n",
      " [0.22 0.28 0.22 0.06 0.36 0.2  0.02 1.   0.24 0.06 0.14 0.24 0.2 ]\n",
      " [0.2  0.26 0.16 0.08 0.32 0.1  0.08 0.24 1.   0.1  0.16 0.12 0.18]\n",
      " [0.1  0.04 0.06 0.12 0.06 0.16 0.1  0.06 0.1  1.   0.12 0.06 0.12]\n",
      " [0.16 0.06 0.1  0.06 0.14 0.18 0.04 0.14 0.16 0.12 1.   0.06 0.12]\n",
      " [0.18 0.16 0.24 0.14 0.18 0.1  0.04 0.24 0.12 0.06 0.06 1.   0.22]\n",
      " [0.14 0.14 0.28 0.1  0.18 0.08 0.04 0.2  0.18 0.12 0.12 0.22 1.  ]]\n",
      "8\n",
      "[[1.   0.06 0.06 0.   0.04 0.04 0.04 0.04]\n",
      " [0.06 1.   0.06 0.08 0.08 0.06 0.08 0.12]\n",
      " [0.06 0.06 1.   0.06 0.06 0.08 0.04 0.06]\n",
      " [0.   0.08 0.06 1.   0.04 0.04 0.1  0.06]\n",
      " [0.04 0.08 0.06 0.04 1.   0.04 0.04 0.08]\n",
      " [0.04 0.06 0.08 0.04 0.04 1.   0.1  0.06]\n",
      " [0.04 0.08 0.04 0.1  0.04 0.1  1.   0.02]\n",
      " [0.04 0.12 0.06 0.06 0.08 0.06 0.02 1.  ]]\n",
      "11\n",
      "[[1.   0.06 0.12 0.   0.06 0.04 0.08 0.04 0.02 0.   0.08]\n",
      " [0.06 1.   0.04 0.06 0.08 0.08 0.04 0.06 0.06 0.04 0.06]\n",
      " [0.12 0.04 1.   0.02 0.12 0.   0.06 0.1  0.06 0.04 0.08]\n",
      " [0.   0.06 0.02 1.   0.02 0.08 0.06 0.02 0.04 0.08 0.04]\n",
      " [0.06 0.08 0.12 0.02 1.   0.12 0.08 0.1  0.04 0.04 0.08]\n",
      " [0.04 0.08 0.   0.08 0.12 1.   0.08 0.06 0.04 0.06 0.04]\n",
      " [0.08 0.04 0.06 0.06 0.08 0.08 1.   0.   0.02 0.04 0.04]\n",
      " [0.04 0.06 0.1  0.02 0.1  0.06 0.   1.   0.02 0.1  0.04]\n",
      " [0.02 0.06 0.06 0.04 0.04 0.04 0.02 0.02 1.   0.   0.06]\n",
      " [0.   0.04 0.04 0.08 0.04 0.06 0.04 0.1  0.   1.   0.06]\n",
      " [0.08 0.06 0.08 0.04 0.08 0.04 0.04 0.04 0.06 0.06 1.  ]]\n",
      "13\n",
      "[[1.   0.04 0.06 0.02 0.04 0.08 0.   0.02 0.08 0.08 0.06 0.06 0.1 ]\n",
      " [0.04 1.   0.06 0.   0.04 0.06 0.06 0.06 0.08 0.06 0.04 0.06 0.04]\n",
      " [0.06 0.06 1.   0.04 0.06 0.06 0.02 0.14 0.06 0.12 0.   0.08 0.  ]\n",
      " [0.02 0.   0.04 1.   0.04 0.06 0.06 0.02 0.02 0.04 0.08 0.1  0.02]\n",
      " [0.04 0.04 0.06 0.04 1.   0.04 0.04 0.08 0.08 0.02 0.04 0.04 0.06]\n",
      " [0.08 0.06 0.06 0.06 0.04 1.   0.04 0.04 0.02 0.06 0.06 0.04 0.08]\n",
      " [0.   0.06 0.02 0.06 0.04 0.04 1.   0.08 0.08 0.02 0.04 0.04 0.06]\n",
      " [0.02 0.06 0.14 0.02 0.08 0.04 0.08 1.   0.06 0.06 0.06 0.02 0.04]\n",
      " [0.08 0.08 0.06 0.02 0.08 0.02 0.08 0.06 1.   0.04 0.04 0.   0.04]\n",
      " [0.08 0.06 0.12 0.04 0.02 0.06 0.02 0.06 0.04 1.   0.02 0.04 0.02]\n",
      " [0.06 0.04 0.   0.08 0.04 0.06 0.04 0.06 0.04 0.02 1.   0.04 0.02]\n",
      " [0.06 0.06 0.08 0.1  0.04 0.04 0.04 0.02 0.   0.04 0.04 1.   0.06]\n",
      " [0.1  0.04 0.   0.02 0.06 0.08 0.06 0.04 0.04 0.02 0.02 0.06 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# ===========================read overlap ratios============================\n",
    "# Dict{attr: Dict{checkpoint: avg_overlap_rate}}\n",
    "avg_overlap_rates: Dict[str, Dict[str, float]] = defaultdict(dict)\n",
    "    \n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "if layers is not None:\n",
    "    print(\"Loop over layers\")\n",
    "    for layer in layers:\n",
    "        embedding = model\n",
    "        experiment_name = f\"inter-layer-{layer}\"\n",
    "        if layer == 25:\n",
    "            experiment_name = \"last-layer\"\n",
    "        DEFAULT_RESULTS_FOLDER = f\"results/{embedding}/{experiment_name}\"\n",
    "\n",
    "        # print(listdir(DEFAULT_RESULTS_FOLDER))\n",
    "        RESULTS = []\n",
    "        rel_treebanks = []\n",
    "        for lang in listdir(DEFAULT_RESULTS_FOLDER):\n",
    "            if lang == 'UD_Chinese-CFL':\n",
    "                continue\n",
    "            rel_treebanks.append(lang)\n",
    "            for attr in listdir(join(DEFAULT_RESULTS_FOLDER, lang)):\n",
    "                RESULTS.append((lang, attr))\n",
    "                \n",
    "        # embedding, attribute, language\n",
    "\n",
    "        DEFAULT_FILE_FORMAT = join(DEFAULT_RESULTS_FOLDER, \"{lang}/{attribute}/loginfo.json\")\n",
    "\n",
    "        results_raw: List[Tuple[Dict[str, Any], List[int]]] = []\n",
    "\n",
    "        for l, a in RESULTS:  # noqa\n",
    "            if l in rel_treebanks:\n",
    "                with open(DEFAULT_FILE_FORMAT.format(lang=l, attribute=a), \"r\") as h:\n",
    "                    data = json.load(h)\n",
    "\n",
    "                results_raw.append(\n",
    "                    (\n",
    "                        { \"embedding\": embedding, \"attribute\": a,\n",
    "                        \"language\": convert_language_code(l) },\n",
    "                        [d[\"iteration_dimension\"] for d in data if \"iteration_dimension\" in d]\n",
    "                        if not random_baseline else random.sample(range(embedding_size), k=top_k)\n",
    "                    )\n",
    "            )\n",
    "        # Compute p values\n",
    "        num_permutations = 1000000\n",
    "        p_vals_cache_file = f\"{embedding}_{top_k}_{num_permutations}_pvals.pkl\"\n",
    "        if not os.path.exists(p_vals_cache_file):\n",
    "            # Compute p-values for different similarities\n",
    "            # if embedding in [\"bert-base-multilingual-cased\", \"xlm-roberta-base\"]:\n",
    "            #     dimensionality = 768\n",
    "            # elif embedding == \"xlm-roberta-large\":\n",
    "            #     dimensionality = 1024\n",
    "            # elif 'bloom' in embedding:\n",
    "            #     dimensionality = 1024\n",
    "            # else:\n",
    "            #     raise Exception(\"Embedding does not exist\")\n",
    "            #     dimensionality = 300\n",
    "            dimensionality = embedding_size\n",
    "\n",
    "            reference_order = random.sample(list(range(dimensionality)), dimensionality)\n",
    "            reference_top_k = reference_order[:top_k]\n",
    "            similarities = []\n",
    "\n",
    "            for i in tqdm(range(num_permutations)):\n",
    "                permuted_top_k = random.sample(reference_order, top_k)\n",
    "                similarities.append(compute_overlap_coefficient(set(reference_top_k), set(permuted_top_k)))\n",
    "                pvals = {}\n",
    "\n",
    "            for i in range(top_k + 1):\n",
    "                observed_hypothesis = i / top_k  # What is overlap score greater than or equal to?\n",
    "                permutations_match = [s for s in similarities if s >= observed_hypothesis]\n",
    "                pval = len(permutations_match) / num_permutations\n",
    "                print(f\"P-value when sim >= {observed_hypothesis} (overlap >= {i} dims): {pval:.5f}\")\n",
    "                pvals[i] = pval\n",
    "\n",
    "            with open(p_vals_cache_file, \"wb\") as h:\n",
    "                pickle.dump(pvals, h)\n",
    "        else:\n",
    "            with open(p_vals_cache_file, \"rb\") as h:\n",
    "                pvals = pickle.load(h)\n",
    "\n",
    "        for attr in attributes:\n",
    "            labels, (similarity_matrix, extra_data) = compute_similarity_for_attribute(\n",
    "                attr, results_raw, top_k)\n",
    "            \n",
    "            # get the average pairwise overlap:\n",
    "            print(len(labels))\n",
    "            print(similarity_matrix)\n",
    "            avg_overlap_rates[attr][layer] = np.average(similarity_matrix[np.triu_indices(len(labels), k = 1)])\n",
    "            \n",
    "\n",
    "            x_labels = labels\n",
    "            y_labels = labels\n",
    "\n",
    "            p_values_matrix = compute_pvalues(extra_data[\"overlap_num\"], pvals)\n",
    "            annotation_matrix = build_statistical_significance_matrix(\n",
    "                p_values_matrix, alpha=0.05, method=\"holm-bonferroni\", symmetry=True)\n",
    "    \n",
    "    for attr, overlap_rates in avg_overlap_rates.items():\n",
    "        overlap_rates = sorted(overlap_rates.items()) \n",
    "        print(overlap_rates)\n",
    "        x, y = zip(*overlap_rates)\n",
    "        \n",
    "        plt.plot(x, y, f'o-', label=attr)\n",
    "\n",
    "\n",
    "elif checkpoints is not None:\n",
    "    print(\"Loop over checkpoints\")\n",
    "    for checkpoint in checkpoints:\n",
    "        embedding = f\"{model}-intermediate-global_step{checkpoint}\"\n",
    "        if checkpoint == 'best':\n",
    "            embedding = model\n",
    "        DEFAULT_RESULTS_FOLDER = f\"results/{embedding}/{experiment_name}\"\n",
    "\n",
    "        # print(listdir(DEFAULT_RESULTS_FOLDER))\n",
    "        RESULTS = []\n",
    "        rel_treebanks = []\n",
    "        for lang in listdir(DEFAULT_RESULTS_FOLDER):\n",
    "            if lang == 'UD_Chinese-CFL':\n",
    "                continue\n",
    "            rel_treebanks.append(lang)\n",
    "            for attr in listdir(join(DEFAULT_RESULTS_FOLDER, lang)):\n",
    "                RESULTS.append((lang, attr))\n",
    "                \n",
    "        # embedding, attribute, language\n",
    "\n",
    "        DEFAULT_FILE_FORMAT = join(DEFAULT_RESULTS_FOLDER, \"{lang}/{attribute}/loginfo.json\")\n",
    "\n",
    "        results_raw: List[Tuple[Dict[str, Any], List[int]]] = []\n",
    "\n",
    "        for l, a in RESULTS:  # noqa\n",
    "            if l in rel_treebanks:\n",
    "                with open(DEFAULT_FILE_FORMAT.format(lang=l, attribute=a), \"r\") as h:\n",
    "                    data = json.load(h)\n",
    "\n",
    "                results_raw.append(\n",
    "                    (\n",
    "                        { \"embedding\": embedding, \"attribute\": a,\n",
    "                        \"language\": convert_language_code(l) },\n",
    "                        [d[\"iteration_dimension\"] for d in data if \"iteration_dimension\" in d]\n",
    "                        if not random_baseline else random.sample(range(embedding_size), k=top_k)\n",
    "                    )\n",
    "            )\n",
    "        # Compute p values\n",
    "        num_permutations = 1000000\n",
    "        p_vals_cache_file = f\"{model}_{top_k}_{num_permutations}_pvals.pkl\"\n",
    "        if not os.path.exists(p_vals_cache_file):\n",
    "            # Compute p-values for different similarities\n",
    "            # if embedding in [\"bert-base-multilingual-cased\", \"xlm-roberta-base\"]:\n",
    "            #     dimensionality = 768\n",
    "            # elif embedding == \"xlm-roberta-large\":\n",
    "            #     dimensionality = 1024\n",
    "            # elif 'bloom' in embedding:\n",
    "            #     dimensionality = 1024\n",
    "            # else:\n",
    "            #     raise Exception(\"Embedding does not exist\")\n",
    "            #     dimensionality = 300\n",
    "            dimensionality = embedding_size\n",
    "\n",
    "            reference_order = random.sample(list(range(dimensionality)), dimensionality)\n",
    "            reference_top_k = reference_order[:top_k]\n",
    "            similarities = []\n",
    "\n",
    "            for i in tqdm(range(num_permutations)):\n",
    "                permuted_top_k = random.sample(reference_order, top_k)\n",
    "                similarities.append(compute_overlap_coefficient(set(reference_top_k), set(permuted_top_k)))\n",
    "                pvals = {}\n",
    "\n",
    "            for i in range(top_k + 1):\n",
    "                observed_hypothesis = i / top_k  # What is overlap score greater than or equal to?\n",
    "                permutations_match = [s for s in similarities if s >= observed_hypothesis]\n",
    "                pval = len(permutations_match) / num_permutations\n",
    "                print(f\"P-value when sim >= {observed_hypothesis} (overlap >= {i} dims): {pval:.5f}\")\n",
    "                pvals[i] = pval\n",
    "\n",
    "            with open(p_vals_cache_file, \"wb\") as h:\n",
    "                pickle.dump(pvals, h)\n",
    "        else:\n",
    "            with open(p_vals_cache_file, \"rb\") as h:\n",
    "                pvals = pickle.load(h)\n",
    "\n",
    "        for attr in attributes:\n",
    "            labels, (similarity_matrix, extra_data) = compute_similarity_for_attribute(\n",
    "                attr, results_raw, top_k)\n",
    "            \n",
    "            # get the average pairwise overlap:\n",
    "            print(len(labels))\n",
    "            print(similarity_matrix)\n",
    "            if checkpoint == 'best':\n",
    "                avg_overlap_rates[attr][checkpoint] = np.average(similarity_matrix[np.triu_indices(len(labels), k = 1)])\n",
    "            else:\n",
    "                avg_overlap_rates[attr][int(checkpoint)] = np.average(similarity_matrix[np.triu_indices(len(labels), k = 1)])\n",
    "\n",
    "            x_labels = labels\n",
    "            y_labels = labels\n",
    "\n",
    "            p_values_matrix = compute_pvalues(extra_data[\"overlap_num\"], pvals)\n",
    "            annotation_matrix = build_statistical_significance_matrix(\n",
    "                p_values_matrix, alpha=0.05, method=\"holm-bonferroni\", symmetry=True)\n",
    "    \n",
    "    for attr, overlap_rates in avg_overlap_rates.items():\n",
    "        \n",
    "        if 'best' in overlap_rates.keys():\n",
    "            rate_for_best_checkpoint = [overlap_rates['best']]*8\n",
    "            del overlap_rates['best']\n",
    "            # plt.plot(x, rate_for_best_checkpoint, f'-{color}', label=f'{model}-{attr}')\n",
    "            \n",
    "        overlap_rates = sorted(overlap_rates.items()) \n",
    "        x, y = zip(*overlap_rates)\n",
    "        \n",
    "        shape = shapes[attr]\n",
    "        \n",
    "        ax2.plot(x, y, f'{shape}--', label=attr)      \n",
    "    \n",
    "else: \n",
    "    print(\"no x axises. please check.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.legend(loc='center right', bbox_to_anchor=(1.5,0.5))\n",
    "# ax1.legend(loc='center left', bbox_to_anchor=(-0.4,0.5))\n",
    "ax2.set_ylabel('overlap rates')\n",
    "# ax1.locator_params(nbins=5, axis='x')\n",
    "fig.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "# fig.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
    "# if layers is not None:\n",
    "#     fig.xlabel(\"layer\")\n",
    "    \n",
    "# # plt.style.use('seaborn-darkgrid')\n",
    "# fig.tight_layout()\n",
    "sns.set_theme()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if layers is not None:\n",
    "    plt.xlabel(\"layer\")\n",
    "    plt.ylabel('neuron overlap rate')\n",
    "    plt.savefig(f'experiments/scatterplots/{model}/layers.pdf', bbox_inches='tight')\n",
    "elif checkpoints is not None:\n",
    "    plt.savefig(f'experiments/scatterplots/{model}/checkpoints_{experiment_name}_f1-score.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-typology-probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed44db9dbf3c575ec3ddf9da9744b6e2502eab3ed89ccbe6b1ade08a3a1a68bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
